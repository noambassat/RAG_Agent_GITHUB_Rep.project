{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMyzZLCAoKiby8os0EvZHBI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noambassat/RAG_project/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain faiss-cpu transformers"
      ],
      "metadata": {
        "id": "QoiJXR-gztif",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5346e007-9099-4e99-e6bc-515e4842d749"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.15)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM3Wsm4ooN2g",
        "outputId": "b707bd43-abbf-40a7-c14e-13c76769ecd9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.39)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "id": "aMSdWm-0n8Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "id": "g4OVx76-lrhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install adjustText"
      ],
      "metadata": {
        "id": "y0K8IEW2z-6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from langchain.vectorstores import FAISS\n",
        "import faiss\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from google.colab import drive\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import json\n",
        "import random\n",
        "import umap\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from adjustText import adjust_text\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "\n",
        "path = \"/content/drive/MyDrive/GitHubRepositoriesProject/\"\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AzOqW6QXWhZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df = pd.read_excel(\"/content/drive/MyDrive/GitHubRepositoriesProject/clean_df.xlsx\")"
      ],
      "metadata": {
        "id": "k9d86mKbjgWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df.shape"
      ],
      "metadata": {
        "id": "MyHqlB3bcey7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "# pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "SjgipgROcFcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the **CustomCodeBERTEmbeddings** Model  \n",
        "The **CodeBERT** model is designed for code-related data. It is based on **BERT** and trained with code data. This model allows generating representations of texts using a **tokenizer** and a **model**.  \n",
        "\n",
        "### The **embed_documents** Function  \n",
        "This function generates embeddings for each given text by:  \n",
        "- **Tokenization:** Breaking down the text into tokens.  \n",
        "- **Text Splitting:** Dividing texts based on length to fit the model’s input limit.  \n",
        "- **Batch Processing:** Producing embeddings in batches to improve efficiency."
      ],
      "metadata": {
        "id": "N-SA04qw1Ffh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "hNShvRVgWLXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCodeBERTEmbeddings(Embeddings):\n",
        "    def __init__(self, model_name=\"microsoft/codebert-base\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "    def embed_documents(self, texts, batch_size=64):  # הוספנו batch_size כארגומנט\n",
        "        embeddings = []\n",
        "\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i + batch_size]\n",
        "            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # שימוש ב-CLS Token\n",
        "            embeddings.extend(batch_embeddings)\n",
        "\n",
        "        return np.array(embeddings)\n",
        "\n",
        "    def embed_query(self, query):\n",
        "        return self.embed_documents([query])[0]\n",
        "\n",
        "embeddings = CustomCodeBERTEmbeddings()\n",
        "\n"
      ],
      "metadata": {
        "id": "_nR5tq08WUlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation  \n",
        "At this stage, the program prepares **all_tokens** and **index_mapping**, allowing me to track which tokens belong to each position in the data.  \n",
        "\n",
        "Then, the **embed_documents** function is used to generate embeddings for the tokens. Each embedding is stored as a vector, and the information is saved in a dictionary called **vector_to_repo** to link each embedding to its original location in the data."
      ],
      "metadata": {
        "id": "A0l4n2zX1QbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_vectors, vector_to_repo = [], {}\n",
        "current_index = 0\n",
        "batch_size = 64\n",
        "\n",
        "# הכנת רשימות שמכילות את כל ה-Tokens מתוך ה-DataFrame\n",
        "all_tokens, index_mapping = [], []\n",
        "for index, topic in tqdm(enumerate(clean_df[\"Topics\"]), total=len(clean_df), desc=\"Extracting Tokens\"):\n",
        "    if not(isinstance(topic, str) and len(topic.strip()) > 0): continue\n",
        "    tokens = [token.strip() for token in topic.split(\",\") if token.strip()]\n",
        "    all_tokens.extend(tokens)\n",
        "    index_mapping.extend([index] * len(tokens))\n",
        "\n",
        "# יצירת האימבדינגס בצורה יעילה עם Batching\n",
        "all_vectors = embeddings.embed_documents(all_tokens, batch_size=batch_size)\n",
        "\n",
        "# שמירת המידע במילון כדי שנוכל לחזור למיקום המקורי בדאטה\n",
        "for i, vector in enumerate(all_vectors):\n",
        "    vector_to_repo[i] = index_mapping[i]\n",
        "\n",
        "# המרת הוקטורים למערך NumPy\n",
        "all_vectors = np.array(all_vectors, dtype='float32')\n"
      ],
      "metadata": {
        "id": "YNBu7zWDYNA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Indexes with FAISS  \n",
        "**FAISS** is a tool optimized for vector-based searches. At this stage, the system creates an **Index** for each different **nlist** group.  \n",
        "\n",
        "Different indexes are built to allow efficient storage and searching over all the generated embeddings.  \n",
        "\n",
        "A new index is constructed for each **nlist**, which is periodically evaluated through searches to select the optimal values."
      ],
      "metadata": {
        "id": "q397P6sl1VUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_optimized_faiss_indexes(all_vectors, nlist_values):\n",
        "    \"\"\"\n",
        "    Creating Different Indexes with FAISS for Various nlist Values\n",
        "    \"\"\"\n",
        "    d = all_vectors.shape[1]\n",
        "    indexes = {}\n",
        "\n",
        "    for nlist in nlist_values:\n",
        "        quantizer = faiss.IndexFlatIP(d)\n",
        "        index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\n",
        "\n",
        "        index.train(all_vectors)\n",
        "        index.add(all_vectors)\n",
        "\n",
        "        indexes[nlist] = index\n",
        "        # print(f\"\\n Created FAISS index with nlist={nlist}. Number of embeddings indexed: {index.ntotal}\")\n",
        "\n",
        "    return indexes\n",
        "\n",
        "\n",
        "nlist_values = [100, 200, 300, 500]\n",
        "indexes = create_optimized_faiss_indexes(all_vectors, nlist_values)\n"
      ],
      "metadata": {
        "id": "hyP03PpkcUMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Searching with FAISS  \n",
        "At this stage, the search is performed within the created indexes. A user query is provided (e.g., \"deep learning\"), and the model searches for the most similar results by comparing embeddings.  \n",
        "\n",
        "The **search_in_index** function searches for the closest words to the query and returns the results along with the words, topics, and relevant links."
      ],
      "metadata": {
        "id": "L5b3EjRz1eKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def search_in_index(query: str, index, top_k: int = 10, nprobe: int = 10):\n",
        "#     \"\"\"\n",
        "#     Advanced Search with FAISS using IndexIVFFlat\n",
        "#     \"\"\"\n",
        "#     index.nprobe = nprobe\n",
        "#     query_vector = embeddings.embed_query(query)\n",
        "\n",
        "#     start_time = time.time()  # מדידת זמן ריצה\n",
        "#     distances, indices = index.search(np.array([query_vector], dtype='float32'), top_k)\n",
        "#     end_time = time.time()\n",
        "\n",
        "#     results = []\n",
        "#     for i, idx in enumerate(indices[0]):\n",
        "#         if idx == -1:\n",
        "#             continue\n",
        "\n",
        "#         repo_index = vector_to_repo[idx]\n",
        "#         repo_name = clean_df.iloc[repo_index][\"Name\"]\n",
        "#         repo_url = clean_df.iloc[repo_index][\"URL\"]\n",
        "#         repo_topics = clean_df.iloc[repo_index][\"Topics\"]\n",
        "#         score = distances[0][i]\n",
        "\n",
        "#         results.append({\n",
        "#             \"name\": repo_name,\n",
        "#             \"url\": repo_url,\n",
        "#             \"topics\": repo_topics,\n",
        "#             \"score\": score\n",
        "#         })\n",
        "\n",
        "#     search_time = end_time - start_time\n",
        "#     return results, search_time\n"
      ],
      "metadata": {
        "id": "6sCmP0jOYpsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid Search  \n",
        "To maximize system performance, a **Grid Search** is performed where different parameter values, such as **nlist** and **nprobe**, are tested.  \n",
        "\n",
        "This process allows me to evaluate search time, the number of relevant results, and filter the results optimally."
      ],
      "metadata": {
        "id": "rpyAN_nC1mpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def grid_search_faiss(queries, indexes, nprobe_values, top_k=10):\n",
        "#     \"\"\"\n",
        "#     Performing Searches for All Combinations of nlist and nprobe\n",
        "#     \"\"\"\n",
        "#     results_summary = []\n",
        "\n",
        "#     for nlist, index in indexes.items():\n",
        "#         for nprobe in nprobe_values:\n",
        "#             for query in queries:\n",
        "#                 search_results, search_time = search_in_index(query, index, top_k=top_k, nprobe=nprobe)\n",
        "\n",
        "\n",
        "#                 found_projects = [result['name'] for result in search_results]\n",
        "#                 found_topics = [result['topics'] for result in search_results]\n",
        "#                 found_urls = [result['url'] for result in search_results]\n",
        "#                 found_scores = [result['score'] for result in search_results]\n",
        "\n",
        "\n",
        "#                 results_summary.append({\n",
        "#                     \"query\": query,\n",
        "#                     \"nlist\": nlist,\n",
        "#                     \"nprobe\": nprobe,\n",
        "#                     \"top_k\": top_k,\n",
        "#                     \"search_time\": search_time,\n",
        "#                     \"relevant_results\": len(found_projects),\n",
        "#                     \"found_projects\": found_projects,\n",
        "#                     \"found_topics\": found_topics,\n",
        "#                     \"found_urls\": found_urls,\n",
        "#                     \"found_scores\": found_scores\n",
        "\n",
        "#                 })\n",
        "\n",
        "\n",
        "#     results_df = pd.DataFrame(results_summary)\n",
        "#     return results_df\n"
      ],
      "metadata": {
        "id": "cOOQkGfzpmsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# queries = [\"deep learning\", \"neural networks\", \"python libraries\", \"computer vision\", \"natural language processing\"]\n",
        "# nprobe_values = [10, 20, 30, 50]\n",
        "\n",
        "# results_df = grid_search_faiss(queries, indexes, nprobe_values, top_k=10)"
      ],
      "metadata": {
        "id": "CJg6Wqyypocf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# results_df.T"
      ],
      "metadata": {
        "id": "1DIjb2QuquhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# results_df.to_excel(\"/content/drive/MyDrive/GitHubRepositoriesProject/results_df.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "LXtv-IOQ3Xam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "81wyNNH7pmYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "path = \"/content/drive/MyDrive/GitHubRepositoriesProject/\"\n",
        "\n",
        "def save_parameters(all_vectors, vector_to_repo, all_tokens, indexes, embeddings):\n",
        "    \"\"\"\n",
        "    Save all the required data to disk.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path + \"saved_data\"):\n",
        "        os.makedirs(path + \"saved_data\")\n",
        "\n",
        "    # Save embeddings and vector_to_repo\n",
        "    with open(path + \"saved_data/embeddings_data.pkl\", \"wb\") as f:\n",
        "        pickle.dump({\n",
        "            \"all_vectors\": all_vectors,\n",
        "            \"vector_to_repo\": vector_to_repo,\n",
        "            \"all_tokens\": all_tokens,\n",
        "            \"embeddings_name\": embeddings.model.config.name_or_path  # Save model name instead of the object\n",
        "        }, f)\n",
        "\n",
        "    # Save FAISS indexes\n",
        "    for nlist, index in indexes.items():\n",
        "        faiss.write_index(index, path + f\"saved_data/faiss_index_{nlist}.index\")\n",
        "\n",
        "    print(\"All data has been successfully saved.\")\n",
        "\n",
        "save_parameters(all_vectors, vector_to_repo, all_tokens, indexes, embeddings)\n"
      ],
      "metadata": {
        "id": "l6xOlwnHHy-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering\n"
      ],
      "metadata": {
        "id": "ZzUvLmrKwFUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MiniBatch KNN"
      ],
      "metadata": {
        "id": "kvMoWCO3wLLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from adjustText import adjust_text\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# ---- 1. הפחתת ממדים בעזרת UMAP ----\n",
        "def reduce_with_umap(all_vectors, n_neighbors=30, min_dist=0.1, n_components=2):\n",
        "    umap_reducer = umap.UMAP(\n",
        "        n_neighbors=n_neighbors,\n",
        "        min_dist=min_dist,\n",
        "        n_components=n_components,\n",
        "        metric='cosine',\n",
        "        random_state=42\n",
        "    )\n",
        "    reduced_embeddings = umap_reducer.fit_transform(all_vectors)\n",
        "    return reduced_embeddings"
      ],
      "metadata": {
        "id": "Y56ryD4QwIFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ---- 2. ביצוע MiniBatch KMeans ----\n",
        "def run_mini_batch_kmeans(reduced_embeddings, n_clusters=20):\n",
        "    kmeans = MiniBatchKMeans(\n",
        "        n_clusters=n_clusters,\n",
        "        batch_size=1000,\n",
        "        random_state=42,\n",
        "        max_iter=300\n",
        "    )\n",
        "    labels = kmeans.fit_predict(reduced_embeddings)\n",
        "    return labels, kmeans\n",
        "\n",
        "# ---- 3. הצגת תוצאות על גרף ----\n",
        "def plot_clusters_with_words(reduced_embeddings, labels, all_tokens, title='MiniBatch KMeans Clustering'):\n",
        "    embedding_df = pd.DataFrame({\n",
        "        'UMAP1': reduced_embeddings[:, 0],\n",
        "        'UMAP2': reduced_embeddings[:, 1],\n",
        "        'Token': all_tokens,\n",
        "        'Cluster': labels\n",
        "    })\n",
        "\n",
        "    # ---- חישוב תדירות מילות מפתח ----\n",
        "    token_frequency = embedding_df['Token'].value_counts()\n",
        "    embedding_df['Frequency'] = embedding_df['Token'].map(token_frequency)\n",
        "\n",
        "    # ---- סינון מונחים לפי שכיחותם ----\n",
        "    threshold_frequency = 5  # הצגת מונחים שמופיעים לפחות מספר פעמים זה\n",
        "    filtered_df = embedding_df[embedding_df['Frequency'] >= threshold_frequency]\n",
        "\n",
        "    # ---- נירמול התדירות בין 0 ל-1 ----\n",
        "    scaler = MinMaxScaler()\n",
        "    embedding_df['Frequency_Scaled'] = scaler.fit_transform(embedding_df[['Frequency']])\n",
        "\n",
        "    # ---- הצגת גרף ----\n",
        "    plt.figure(figsize=(18, 12))\n",
        "    sns.scatterplot(\n",
        "        data=embedding_df,\n",
        "        x='UMAP1', y='UMAP2',\n",
        "        hue='Cluster',\n",
        "        palette='tab20',\n",
        "        legend='full',\n",
        "        alpha=0.6\n",
        "    )\n",
        "\n",
        "    previous_tokens = set()  # רשימה לשמירה על המילים שכבר הוצגו\n",
        "    texts = []  # רשימה של הטקסטים שמוסיפים לגרף\n",
        "\n",
        "    # הוספת טקסטים למרכזי קלאסטרים\n",
        "    for cluster_label in embedding_df['Cluster'].unique():\n",
        "        cluster_df = filtered_df[filtered_df['Cluster'] == cluster_label]\n",
        "\n",
        "        # סידור לפי תדירות המילים בתוך כל קלאסטר\n",
        "        top_words = cluster_df.groupby('Token')['Frequency'].sum().sort_values(ascending=False)\n",
        "\n",
        "        # בחירת מילה הכי שכיחה שאינה חופפת למילים שכבר הודפסו\n",
        "        for token, freq in top_words.items():\n",
        "            cluster_center = cluster_df[['UMAP1', 'UMAP2']].mean()\n",
        "            if token not in previous_tokens:\n",
        "                texts.append(plt.text(\n",
        "                    cluster_center['UMAP1'],\n",
        "                    cluster_center['UMAP2'],\n",
        "                    token,\n",
        "                    fontsize=min(20, 8 + freq / 2),\n",
        "                    alpha=0.9,\n",
        "                    weight='bold' if freq > 50 else 'normal'\n",
        "                ))\n",
        "                previous_tokens.add(token)\n",
        "                break  # לקחת רק את המילה הכי שכיחה לקלאסטר\n",
        "\n",
        "    # התאמת המילים מבלי לחפוף אחת על השנייה\n",
        "    adjust_text(texts)\n",
        "\n",
        "    plt.title(title, fontsize=22)\n",
        "    plt.xlabel('UMAP Dimension 1', fontsize=16)\n",
        "    plt.ylabel('UMAP Dimension 2', fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "    return embedding_df\n",
        "\n",
        "\n",
        "# ---- 4. הפחתת ממדים עם כל האימבדינגס ----\n",
        "reduced_embeddings = reduce_with_umap(all_vectors)\n",
        "\n",
        "# ---- 5. קלאסטרינג מלא על כל האימבדינגס ----\n",
        "labels, kmeans_model = run_mini_batch_kmeans(reduced_embeddings, n_clusters=20)\n",
        "\n",
        "# ---- 6. הצגת תוצאות עם מילים מתויגות ----\n",
        "embedding_df = plot_clusters_with_words(reduced_embeddings, labels, all_tokens)\n"
      ],
      "metadata": {
        "id": "c8np4frBoZYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HDBSCAN"
      ],
      "metadata": {
        "id": "6dXoO3eEwWa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import hdbscan\n",
        "# import torch\n",
        "# import umap\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from adjustText import adjust_text\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# # ---- 1. הפחתת ממדים בעזרת UMAP ----\n",
        "# def reduce_with_umap(all_vectors, n_neighbors=30, min_dist=0.1, n_components=2):\n",
        "#     all_vectors = torch.tensor(all_vectors).to(device)  # Load to GPU if available\n",
        "#     umap_reducer = umap.UMAP(\n",
        "#         n_neighbors=n_neighbors,\n",
        "#         min_dist=min_dist,\n",
        "#         n_components=n_components,\n",
        "#         metric='cosine',\n",
        "#         random_state=42\n",
        "#     )\n",
        "#     reduced_embeddings = umap_reducer.fit_transform(all_vectors.cpu().numpy())\n",
        "#     return reduced_embeddings\n",
        "\n",
        "# # ---- 2. ביצוע HDBSCAN ----\n",
        "# def run_hdbscan(reduced_embeddings, min_cluster_size=30, min_samples=10):\n",
        "#     clusterer = hdbscan.HDBSCAN(\n",
        "#         min_cluster_size=min_cluster_size,\n",
        "#         min_samples=min_samples,\n",
        "#         core_dist_n_jobs=1  # Deactivate parallel processing\n",
        "#     )\n",
        "#     labels = clusterer.fit_predict(reduced_embeddings)\n",
        "#     return labels, clusterer\n",
        "\n",
        "# #\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def plot_clusters_with_words(reduced_embeddings, labels, all_tokens):\n",
        "#     \"\"\"\n",
        "#     Plot clusters with most frequent words only for better efficiency.\n",
        "#     \"\"\"\n",
        "#     # ---- יצירת DataFrame עם האימבדינגס והקלאסטרים ----\n",
        "#     embedding_df = pd.DataFrame({\n",
        "#         'UMAP1': reduced_embeddings[:, 0],\n",
        "#         'UMAP2': reduced_embeddings[:, 1],\n",
        "#         'Token': all_tokens,\n",
        "#         'Cluster': labels\n",
        "#     })\n",
        "\n",
        "#     # ---- חישוב שכיחות מילות מפתח ----\n",
        "#     token_frequency = embedding_df['Token'].value_counts()\n",
        "#     embedding_df['Frequency'] = embedding_df['Token'].map(token_frequency)\n",
        "\n",
        "#     # ---- סינון מילים עם שכיחות גבוהה (מעל סף מסוים) ----\n",
        "#     threshold_frequency = 5  # את יכולה לשנות את המספר הזה כרצונך\n",
        "#     filtered_df = embedding_df[embedding_df['Frequency'] >= threshold_frequency]\n",
        "\n",
        "#     # ---- נירמול התדירות בין 0 ל-1 בשביל צבעים ----\n",
        "#     scaler = MinMaxScaler()\n",
        "#     embedding_df['Frequency_Scaled'] = scaler.fit_transform(embedding_df[['Frequency']])\n",
        "\n",
        "#     # ---- גרף ----\n",
        "#     plt.figure(figsize=(18, 12))\n",
        "#     sns.scatterplot(\n",
        "#         data=embedding_df,\n",
        "#         x='UMAP1',\n",
        "#         y='UMAP2',\n",
        "#         hue='Cluster',\n",
        "#         palette='tab20',\n",
        "#         alpha=0.6,\n",
        "#         legend='full'\n",
        "#     )\n",
        "\n",
        "#     # ---- הוספת שמות של מילים תדירות בלבד ----\n",
        "#     texts = []\n",
        "#     previous_tokens = set()\n",
        "#     for _, row in filtered_df.iterrows():\n",
        "#         token = row['Token']\n",
        "#         if token not in previous_tokens:\n",
        "#             texts.append(plt.text(\n",
        "#                 row['UMAP1'],\n",
        "#                 row['UMAP2'],\n",
        "#                 token,\n",
        "#                 fontsize=min(20, 8 + row['Frequency'] / 2),\n",
        "#                 alpha=0.75\n",
        "#             ))\n",
        "#             previous_tokens.add(token)\n",
        "\n",
        "#     adjust_text(texts)\n",
        "#     plt.title('Clustering with Word Frequency', fontsize=22)\n",
        "#     plt.xlabel('UMAP Dimension 1', fontsize=16)\n",
        "#     plt.ylabel('UMAP Dimension 2', fontsize=16)\n",
        "#     plt.show()\n",
        "\n",
        "#     return embedding_df\n",
        "\n",
        "\n",
        "# # ---- 4. הרצה של HDBSCAN ----\n",
        "\n",
        "# labels, hdbscan_model = run_hdbscan(reduced_embeddings)\n",
        "# embedding_df_hdbscan = plot_clusters_with_words(reduced_embeddings, labels, all_tokens)\n"
      ],
      "metadata": {
        "id": "h294N6j-wXvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GMM (Gaussian Mixture Model)"
      ],
      "metadata": {
        "id": "U8sMNq2dwWsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from adjustText import adjust_text\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- 1. ביצוע GMM ----\n",
        "def run_gmm(reduced_embeddings, n_components=20):\n",
        "    gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)\n",
        "    labels = gmm.fit_predict(reduced_embeddings)\n",
        "    return labels, gmm\n",
        "\n",
        "# ---- 2. גרף עם מילים תדירות ----\n",
        "def plot_gmm_with_words(reduced_embeddings, labels, all_tokens, threshold_frequency=5):\n",
        "    \"\"\"\n",
        "    Plot GMM clusters with most frequent words only for better efficiency.\n",
        "    \"\"\"\n",
        "    # ---- יצירת DataFrame עם האימבדינגס והקלאסטרים ----\n",
        "    embedding_df = pd.DataFrame({\n",
        "        'UMAP1': reduced_embeddings[:, 0],\n",
        "        'UMAP2': reduced_embeddings[:, 1],\n",
        "        'Token': all_tokens,\n",
        "        'Cluster': labels\n",
        "    })\n",
        "\n",
        "    # ---- חישוב שכיחות מילות מפתח ----\n",
        "    token_frequency = embedding_df['Token'].value_counts()\n",
        "    embedding_df['Frequency'] = embedding_df['Token'].map(token_frequency)\n",
        "\n",
        "    # ---- סינון מילים עם שכיחות גבוהה בלבד ----\n",
        "    filtered_df = embedding_df[embedding_df['Frequency'] >= threshold_frequency]\n",
        "\n",
        "    # ---- נירמול התדירות לצבעים ----\n",
        "    scaler = MinMaxScaler()\n",
        "    embedding_df['Frequency_Scaled'] = scaler.fit_transform(embedding_df[['Frequency']])\n",
        "\n",
        "    # ---- גרף ----\n",
        "    plt.figure(figsize=(18, 12))\n",
        "    sns.scatterplot(\n",
        "        data=embedding_df,\n",
        "        x='UMAP1',\n",
        "        y='UMAP2',\n",
        "        hue='Cluster',\n",
        "        palette='tab20',\n",
        "        alpha=0.6,\n",
        "        legend='full'\n",
        "    )\n",
        "\n",
        "    # ---- הוספת שמות מילים שכיחות בלבד ----\n",
        "    texts = []\n",
        "    previous_tokens = set()  # רשימה של מילים שכבר הודפסו כדי למנוע כפילויות\n",
        "    for _, row in filtered_df.iterrows():\n",
        "        token = row['Token']\n",
        "        if token not in previous_tokens:\n",
        "            texts.append(plt.text(\n",
        "                row['UMAP1'],\n",
        "                row['UMAP2'],\n",
        "                token,\n",
        "                fontsize=min(20, 8 + row['Frequency'] / 2),\n",
        "                alpha=0.75\n",
        "            ))\n",
        "            previous_tokens.add(token)\n",
        "\n",
        "    adjust_text(texts)\n",
        "    plt.title('GMM Clustering with Word Frequency', fontsize=22)\n",
        "    plt.xlabel('UMAP Dimension 1', fontsize=16)\n",
        "    plt.ylabel('UMAP Dimension 2', fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "    return embedding_df\n"
      ],
      "metadata": {
        "id": "nKXI6suVwjz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 3. הרצה של GMM ----\n",
        "labels_gmm, gmm_model = run_gmm(reduced_embeddings)\n",
        "embedding_df_gmm = plot_gmm_with_words(reduced_embeddings, labels_gmm, all_tokens)\n"
      ],
      "metadata": {
        "id": "NenH2Hq1M0Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9WFwb12BM0RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dBf2AXaaM0Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# import os\n",
        "\n",
        "# path = \"/content/drive/MyDrive/GitHubRepositoriesProject/saved_data/\"\n",
        "\n",
        "# def save_all_parameters_and_models(\n",
        "#     all_vectors, vector_to_repo, all_tokens, embeddings, best_params=None,\n",
        "#     kmeans_model, hdbscan_model, gmm_model, indexes\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     Save all relevant parameters and models to disk.\n",
        "#     \"\"\"\n",
        "#     if not os.path.exists(path):\n",
        "#         os.makedirs(path)\n",
        "\n",
        "#     # ---- שמירת אינדקסי FAISS ----\n",
        "#     for nlist, index in indexes.items():\n",
        "#         faiss.write_index(index, f\"{path}faiss_index_{nlist}.index\")\n",
        "\n",
        "#     # ---- שמירת כל הנתונים בעזרת פיקל ----\n",
        "#     data = {\n",
        "#         \"vector_to_repo\": vector_to_repo,\n",
        "#         \"all_vectors\": all_vectors,\n",
        "#         \"all_tokens\": all_tokens,\n",
        "#         \"embeddings_name\": embeddings.model.config.name_or_path,\n",
        "#         \"best_params\": best_params,\n",
        "#         \"kmeans_model\": kmeans_model,\n",
        "#         \"hdbscan_model\": hdbscan_model,\n",
        "#         \"gmm_model\": gmm_model\n",
        "#     }\n",
        "\n",
        "#     with open(f\"{path}all_parameters_and_models.pkl\", \"wb\") as f:\n",
        "#         pickle.dump(data, f)\n",
        "\n",
        "#     print(\"All parameters and models saved successfully.\")\n",
        "\n",
        "# # ---- קריאה לפונקציה כדי לשמור הכל ----\n",
        "# save_all_parameters_and_models(\n",
        "#     all_vectors=all_vectors,\n",
        "#     vector_to_repo=vector_to_repo,\n",
        "#     all_tokens=all_tokens,\n",
        "#     embeddings=embeddings,\n",
        "#     best_params=best_params if best_params else None,\n",
        "#     kmeans_model=kmeans_model,\n",
        "#     # hdbscan_model=hdbscan_model,\n",
        "#     gmm_model=gmm_model,\n",
        "#     indexes=indexes\n",
        "# )\n"
      ],
      "metadata": {
        "id": "jLe1jnmuLVKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8rkEih-XIthF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nlwjBXUnwWul"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q_hD_1lfJCNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### CONTINUE"
      ],
      "metadata": {
        "id": "tf4-M6qwJCPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_all_parameters_and_models(nlist_values):\n",
        "    \"\"\"\n",
        "    Load all relevant parameters and models from disk.\n",
        "    \"\"\"\n",
        "    indexes = {}\n",
        "    for nlist in nlist_values:\n",
        "        index_path = f\"{path}faiss_index_{nlist}.index\"\n",
        "        if os.path.exists(index_path):\n",
        "            index = faiss.read_index(index_path)\n",
        "            indexes[nlist] = index\n",
        "        else:\n",
        "            print(f\"Index file for nlist={nlist} not found at {index_path}.\")\n",
        "\n",
        "    data_path = f\"{path}all_parameters_and_models.pkl\"\n",
        "    if os.path.exists(data_path):\n",
        "        with open(data_path, \"rb\") as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        vector_to_repo = data[\"vector_to_repo\"]\n",
        "        all_vectors = data[\"all_vectors\"]\n",
        "        all_tokens = data[\"all_tokens\"]\n",
        "        embeddings = CustomCodeBERTEmbeddings(model_name=data[\"embeddings_name\"])\n",
        "        best_params = data[\"best_params\"]\n",
        "        kmeans_model = data[\"kmeans_model\"]\n",
        "        hdbscan_model = data[\"hdbscan_model\"]\n",
        "        gmm_model = data[\"gmm_model\"]\n",
        "\n",
        "        print(\"All parameters and models loaded successfully.\")\n",
        "        return indexes, vector_to_repo, all_vectors, all_tokens, embeddings, best_params, kmeans_model, hdbscan_model, gmm_model\n",
        "    else:\n",
        "        print(f\"Precomputed data file not found at {data_path}.\")\n",
        "        return None, None, None, None, None, None, None, None, None\n",
        "\n",
        "\n",
        "loaded_data = load_all_parameters_and_models()\n"
      ],
      "metadata": {
        "id": "CY2QnJ84JCSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Analysis and Visualization\n"
      ],
      "metadata": {
        "id": "aecoGdbp2shd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_similarity(query_vector, all_vectors):\n",
        "    \"\"\"\n",
        "    Calculate cosine similarity between the query vector and all_vectors.\n",
        "    Returns a list of similarity scores.\n",
        "    \"\"\"\n",
        "    similarities = cosine_similarity(np.array([query_vector]), all_vectors)\n",
        "    return similarities.flatten()\n",
        "\n",
        "\n",
        "\n",
        "def build_ground_truth(queries, embeddings, all_vectors, vector_to_repo, clean_df,\n",
        "                                top_k_percent=0.1, percentile=90, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Build ground truth based on multiple criteria for relevance determination.\n",
        "\n",
        "    Parameters:\n",
        "    - queries: List of query strings.\n",
        "    - embeddings: The embedding model used (CustomCodeBERTEmbeddings).\n",
        "    - all_vectors: The FAISS embeddings for the projects.\n",
        "    - vector_to_repo: Mapping from vector index to repo index.\n",
        "    - clean_df: The DataFrame containing the project metadata.\n",
        "    - top_k_percent: Percentage for Top K% filtering (e.g., 0.1 for Top 10%).\n",
        "    - percentile: Percentile threshold for filtering (e.g., 90 for 90th percentile).\n",
        "    - alpha: Factor for Dynamic Threshold calculation (Mean + alpha * Std).\n",
        "\n",
        "    Returns:\n",
        "    - advanced_ground_truth_df: A DataFrame containing relevant results for each query.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for query in queries:\n",
        "        query_vector = embeddings.embed_query(query)\n",
        "        similarities = calculate_similarity(query_vector, all_vectors)\n",
        "\n",
        "        # Top K% Filtering\n",
        "        top_k_threshold = np.percentile(similarities, 100 - (top_k_percent * 100))\n",
        "        top_k_indices = np.where(similarities >= top_k_threshold)[0]\n",
        "\n",
        "        # Percentile-based Filtering\n",
        "        percentile_threshold = np.percentile(similarities, percentile)\n",
        "        percentile_indices = np.where(similarities >= percentile_threshold)[0]\n",
        "\n",
        "        # Dynamic Threshold Filtering\n",
        "        mean_similarity = np.mean(similarities)\n",
        "        std_similarity = np.std(similarities)\n",
        "        dynamic_threshold = mean_similarity + alpha * std_similarity\n",
        "        dynamic_indices = np.where(similarities >= dynamic_threshold)[0]\n",
        "\n",
        "        # Combine all indices\n",
        "        all_relevant_indices = np.unique(np.concatenate((top_k_indices, percentile_indices, dynamic_indices)))\n",
        "\n",
        "        for idx in all_relevant_indices:\n",
        "            repo_index = vector_to_repo[idx]\n",
        "            repo_name = clean_df.iloc[repo_index][\"Name\"]\n",
        "            repo_url = clean_df.iloc[repo_index][\"URL\"]\n",
        "            repo_topics = clean_df.iloc[repo_index][\"Topics\"]\n",
        "            score = similarities[idx]\n",
        "\n",
        "            results.append({\n",
        "                \"query\": query,\n",
        "                \"repo_name\": repo_name,\n",
        "                \"repo_url\": repo_url,\n",
        "                \"repo_topics\": repo_topics,\n",
        "                \"score\": score\n",
        "            })\n",
        "\n",
        "    # Create DataFrame for Ground Truth\n",
        "    advanced_ground_truth_df = pd.DataFrame(results)\n",
        "    return advanced_ground_truth_df\n",
        "queries = [\"deep learning\", \"neural networks\", \"python libraries\", \"computer vision\", \"natural language processing\"]\n",
        "\n",
        "ground_truth_df = build_ground_truth(\n",
        "    queries=queries,\n",
        "    embeddings=embeddings,\n",
        "    all_vectors=all_vectors,\n",
        "    vector_to_repo=vector_to_repo,\n",
        "    clean_df=clean_df,\n",
        "    top_k_percent=0.1,\n",
        "    percentile=90,\n",
        "    alpha=0.5\n",
        ")\n",
        "\n",
        "ground_truth_df.head(10)\n",
        "\n"
      ],
      "metadata": {
        "id": "2ROUIhQDHax-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "USsp8mkWm0sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_similarity(query_vector, all_vectors):\n",
        "    \"\"\"\n",
        "    Calculate cosine similarity between the query vector and all_vectors using GPU if available.\n",
        "    \"\"\"\n",
        "    if device == 'cuda':\n",
        "        query_vector = torch.tensor(query_vector).to(device)\n",
        "        all_vectors_gpu = torch.tensor(all_vectors).to(device)\n",
        "        similarities = torch.nn.functional.cosine_similarity(query_vector.unsqueeze(0), all_vectors_gpu)\n",
        "        return similarities.cpu().numpy()\n",
        "    else:\n",
        "        return cosine_similarity(np.array([query_vector]), all_vectors).flatten()\n",
        "\n",
        "\n",
        "def build_ground_truth(queries, embeddings, all_vectors, vector_to_repo, clean_df,\n",
        "                       top_k_percent, percentile, alpha):\n",
        "    \"\"\"\n",
        "    Build ground truth based on multiple criteria for relevance determination.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for query in queries:\n",
        "        query_vector = embeddings.embed_query(query)\n",
        "        similarities = calculate_similarity(query_vector, all_vectors)\n",
        "\n",
        "        # Top K% Filtering\n",
        "        top_k_threshold = np.percentile(similarities, 100 - (top_k_percent * 100))\n",
        "        top_k_indices = np.where(similarities >= top_k_threshold)[0]\n",
        "\n",
        "        # Percentile-based Filtering\n",
        "        percentile_threshold = np.percentile(similarities, percentile)\n",
        "        percentile_indices = np.where(similarities >= percentile_threshold)[0]\n",
        "\n",
        "        # Dynamic Threshold Filtering\n",
        "        mean_similarity = np.mean(similarities)\n",
        "        std_similarity = np.std(similarities)\n",
        "        dynamic_threshold = mean_similarity + alpha * std_similarity\n",
        "        dynamic_indices = np.where(similarities >= dynamic_threshold)[0]\n",
        "\n",
        "        # Combine all indices\n",
        "        all_relevant_indices = np.unique(np.concatenate((top_k_indices, percentile_indices, dynamic_indices)))\n",
        "\n",
        "        for idx in all_relevant_indices:\n",
        "            repo_index = vector_to_repo[idx]\n",
        "            repo_name = clean_df.iloc[repo_index][\"Name\"]\n",
        "            repo_url = clean_df.iloc[repo_index][\"URL\"]\n",
        "            repo_topics = clean_df.iloc[repo_index][\"Topics\"]\n",
        "            score = similarities[idx]\n",
        "\n",
        "            results.append({\n",
        "                \"query\": query,\n",
        "                \"repo_name\": repo_name,\n",
        "                \"repo_url\": repo_url,\n",
        "                \"repo_topics\": repo_topics,\n",
        "                \"score\": score\n",
        "            })\n",
        "\n",
        "    ground_truth_df = pd.DataFrame(results)\n",
        "    return ground_truth_df\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"\n",
        "    Optuna objective function to optimize the ground truth generation process.\n",
        "    \"\"\"\n",
        "    top_k_percent = trial.suggest_float(\"top_k_percent\", 0.01, 0.5)  # Between 1% to 50%\n",
        "    percentile = trial.suggest_int(\"percentile\", 80, 99)  # Between 80th to 99th percentile\n",
        "    alpha = trial.suggest_float(\"alpha\", 0.0, 2.0)  # Between 0 and 2 (for dynamic threshold)\n",
        "\n",
        "    # Build the ground truth based on the current hyperparameters\n",
        "    ground_truth_df = build_ground_truth(\n",
        "        queries=queries,\n",
        "        embeddings=embeddings,\n",
        "        all_vectors=all_vectors,\n",
        "        vector_to_repo=vector_to_repo,\n",
        "        clean_df=clean_df,\n",
        "        top_k_percent=top_k_percent,\n",
        "        percentile=percentile,\n",
        "        alpha=alpha\n",
        "    )\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    precision_scores, recall_scores, f1_scores = [], [], []\n",
        "    for query in queries:\n",
        "        relevant_results = ground_truth_df[ground_truth_df['query'] == query]\n",
        "\n",
        "        if relevant_results.empty:\n",
        "            continue\n",
        "\n",
        "        y_true = [1] * len(relevant_results)\n",
        "        y_pred = [1] * len(relevant_results)  # Assuming all found are relevant\n",
        "\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    # Return average F1-score\n",
        "    return np.mean(f1_scores)\n",
        "\n",
        "\n",
        "# ---- הרצה של Optuna ----\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# הצגת התוצאות הטובות ביותר\n",
        "print(\"Best params:\", study.best_params)\n",
        "print(\"Best F1 Score:\", study.best_value)\n"
      ],
      "metadata": {
        "id": "jGUxw4d8QaiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimal Parameters Found by Optuna for Building the Ground Truth  \n",
        "\n",
        "The following parameters were found to be optimal for constructing the ground truth:  \n",
        "\n",
        "- **top_k_percent:** 0.1449 (Top 14.49% of the results).  \n",
        "- **percentile:** 91 (Results in the 91st percentile and above).  \n",
        "- **alpha:** 0.4516 (Affects the Dynamic Threshold).  \n",
        "\n",
        "### Average F1 Score: 1.0  \n",
        "This indicates that the method perfectly identifies all relevant results for the defined queries.  \n",
        "\n",
        "### Conclusion:  \n",
        "The approach of combining three filters (**Top K%**, **Percentiles**, and **Dynamic Threshold**) has proven to be efficient and accurate."
      ],
      "metadata": {
        "id": "s-GI2Zm3WD4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Searching with Optimal Parameters\n",
        "Using the parameters found by Optuna to perform the actual search.\n",
        "\n",
        "Using nlist and nprobe\n",
        "Currently setting nprobe = 10. I will evaluate later if it needs further optimization."
      ],
      "metadata": {
        "id": "Ox12gWZYWxid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def search_with_optimized_params(queries, indexes, vector_to_repo, clean_df, embeddings, top_k=10):\n",
        "    \"\"\"\n",
        "    Perform search using the optimized parameters found with Optuna.\n",
        "\n",
        "    Parameters:\n",
        "    - queries: List of search queries.\n",
        "    - indexes: Dictionary of FAISS indexes (key: nlist, value: index object).\n",
        "    - vector_to_repo: Mapping from vector index to repo index.\n",
        "    - clean_df: DataFrame containing repository information.\n",
        "    - embeddings: The embedding model to convert queries to vectors.\n",
        "    - top_k: Number of top results to retrieve per query.\n",
        "\n",
        "    Returns:\n",
        "    - results_df: DataFrame containing search results.\n",
        "    \"\"\"\n",
        "    results_summary = []\n",
        "\n",
        "    for nlist, index in indexes.items():\n",
        "        index.nprobe = 10  # Using a fixed nprobe for now, we can optimize this later\n",
        "\n",
        "        for query in tqdm(queries, desc=f\"Searching with nlist={nlist}\"):\n",
        "            query_vector = embeddings.embed_query(query)\n",
        "            distances, indices = index.search(np.array([query_vector], dtype='float32'), top_k)\n",
        "\n",
        "            for i, idx in enumerate(indices[0]):\n",
        "                if idx == -1:\n",
        "                    continue\n",
        "\n",
        "                repo_index = vector_to_repo[idx]\n",
        "                repo_name = clean_df.iloc[repo_index][\"Name\"]\n",
        "                repo_url = clean_df.iloc[repo_index][\"URL\"]\n",
        "                repo_topics = clean_df.iloc[repo_index][\"Topics\"]\n",
        "                score = distances[0][i]\n",
        "\n",
        "                results_summary.append({\n",
        "                    \"query\": query,\n",
        "                    \"nlist\": nlist,\n",
        "                    \"repo_name\": repo_name,\n",
        "                    \"repo_url\": repo_url,\n",
        "                    \"repo_topics\": repo_topics,\n",
        "                    \"score\": score\n",
        "                })\n",
        "\n",
        "    # Create DataFrame with the search results\n",
        "    results_df = pd.DataFrame(results_summary)\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# ---- חיפוש בעזרת הפרמטרים האופטימליים ----\n",
        "optimized_results_df = search_with_optimized_params(\n",
        "    queries=queries,\n",
        "    indexes=indexes,\n",
        "    vector_to_repo=vector_to_repo,\n",
        "    clean_df=clean_df,\n",
        "    embeddings=embeddings,\n",
        "    top_k=10\n",
        ")\n",
        "\n",
        "# הצגת תוצאות החיפוש\n",
        "optimized_results_df.head(10)\n"
      ],
      "metadata": {
        "id": "TQuZYuTo7Lch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "#### Building Ground Truth  \n",
        "Checking which projects are relevant to the queries according to **ground_truth_df** created using **Optuna**.  \n",
        "\n",
        "#### FAISS Search Results  \n",
        "Comparing the projects retrieved by **FAISS** against the **Ground Truth**.  \n",
        "\n",
        "#### Metrics Calculation  \n",
        "Calculating **Precision**, **Recall**, and **F1-Score** for each query separately.  \n",
        "\n",
        "#### Displaying Results  \n",
        "The results are presented in a table, showing for each query how accurately **FAISS** identifies the relevant projects."
      ],
      "metadata": {
        "id": "9L3SJYwlWWMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_search_results(optimized_results_df, ground_truth_df):\n",
        "    \"\"\"\n",
        "    Compare search results from FAISS with the ground truth and calculate metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - optimized_results_df: DataFrame containing search results from FAISS.\n",
        "    - ground_truth_df: DataFrame containing the ground truth results.\n",
        "\n",
        "    Returns:\n",
        "    - metrics_df: DataFrame containing precision, recall, and f1-score for each query.\n",
        "    \"\"\"\n",
        "    precision_list, recall_list, f1_list = [],[],[]\n",
        "    queries = optimized_results_df['query'].unique()\n",
        "\n",
        "    for query in queries:\n",
        "        # Get the relevant projects according to ground truth\n",
        "        relevant_projects = set(ground_truth_df[ground_truth_df['query'] == query]['repo_name'])\n",
        "\n",
        "        # Get the projects retrieved by FAISS\n",
        "        retrieved_projects = set(optimized_results_df[optimized_results_df['query'] == query]['repo_name'])\n",
        "\n",
        "        # Create binary labels\n",
        "        y_true = [1 if project in relevant_projects else 0 for project in retrieved_projects]\n",
        "        y_pred = [1] * len(retrieved_projects)\n",
        "\n",
        "        # Calculate metrics\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "        # Store results\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "        f1_list.append(f1)\n",
        "\n",
        "    # Create DataFrame to display results\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'query': queries,\n",
        "        'precision': precision_list,\n",
        "        'recall': recall_list,\n",
        "        'f1': f1_list\n",
        "    })\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "\n",
        "metrics_df = evaluate_search_results(optimized_results_df, ground_truth_df)"
      ],
      "metadata": {
        "id": "u28HIQer8lIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df"
      ],
      "metadata": {
        "id": "gQJ04QvwWjoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = study.best_params\n",
        "top_k_percent = best_params[\"top_k_percent\"]\n",
        "percentile = best_params[\"percentile\"]\n",
        "alpha = best_params[\"alpha\"]\n"
      ],
      "metadata": {
        "id": "YTSavlEMuCzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ---- 1. דגימה של תתי-וקטורים ----\n",
        "sample_size = 5000\n",
        "sample_indices = random.sample(range(len(all_vectors)), min(sample_size, len(all_vectors)))\n",
        "sampled_vectors = [all_vectors[i] for i in sample_indices]\n",
        "sampled_tokens = [all_tokens[i] for i in sample_indices]\n",
        "\n",
        "# ---- 2. הפחתת ממדים בעזרת UMAP ----\n",
        "umap_reducer = umap.UMAP(\n",
        "    n_neighbors=int(30 * top_k_percent),  # התאמת הפרמטר top_k_percent\n",
        "    n_components=2,\n",
        "    metric='cosine',\n",
        "    random_state=42,\n",
        "    min_dist=0.1\n",
        ")\n",
        "reduced_embeddings = umap_reducer.fit_transform(sampled_vectors)\n",
        "\n",
        "# ---- 3. הכנת הנתונים להצגה ----\n",
        "embedding_df = pd.DataFrame({\n",
        "    'UMAP1': reduced_embeddings[:, 0],\n",
        "    'UMAP2': reduced_embeddings[:, 1],\n",
        "    'Token': sampled_tokens\n",
        "})\n",
        "\n",
        "# ---- 4. חישוב תדירות מילות מפתח ----\n",
        "token_frequency = embedding_df['Token'].value_counts()\n",
        "embedding_df['Frequency'] = embedding_df['Token'].map(token_frequency)\n",
        "\n",
        "# ---- 5. סינון מונחים לפי שכיחותם ----\n",
        "threshold_frequency = 5  # הצגת מונחים שמופיעים לפחות מספר פעמים זה\n",
        "filtered_df = embedding_df[embedding_df['Frequency'] >= threshold_frequency]\n",
        "\n",
        "# ---- 6. נירמול התדירות בין 0 ל-1 ----\n",
        "scaler = MinMaxScaler()\n",
        "embedding_df['Frequency_Scaled'] = scaler.fit_transform(embedding_df[['Frequency']])\n",
        "\n",
        "# ---- 7. הגדרות גרף ----\n",
        "plt.figure(figsize=(18, 12))  # גודל גרף מותאם\n",
        "\n",
        "# הגדרת הגרף עם ax\n",
        "ax = sns.scatterplot(\n",
        "    x='UMAP1',\n",
        "    y='UMAP2',\n",
        "    hue='Frequency_Scaled',\n",
        "    palette='cool',\n",
        "    data=embedding_df,\n",
        "    s=80,\n",
        "    alpha=0.7\n",
        ")\n",
        "\n",
        "# הצגת מפת צבעים\n",
        "norm = plt.Normalize(embedding_df['Frequency_Scaled'].min(), embedding_df['Frequency_Scaled'].max())\n",
        "sm = plt.cm.ScalarMappable(cmap='cool', norm=norm)\n",
        "sm.set_array([])\n",
        "plt.colorbar(sm, ax=ax, label='Token Frequency (Scaled)', orientation='vertical')\n",
        "\n",
        "# ---- 8. הוספת שמות המילים ----\n",
        "previous_tokens = set()  # רשימה לשמירה על המילים שכבר הוצגו\n",
        "texts = []  # רשימה של הטקסטים שמוסיפים לגרף\n",
        "for i, row in filtered_df.iterrows():\n",
        "    x, y = row['UMAP1'], row['UMAP2']\n",
        "    token = row['Token']\n",
        "\n",
        "    if token not in previous_tokens:\n",
        "        texts.append(plt.text(\n",
        "            x,\n",
        "            y,\n",
        "            token,\n",
        "            fontsize=min(20, 8 + row['Frequency'] / 2),\n",
        "            alpha=0.9,\n",
        "            weight='bold' if row['Frequency'] > 50 else 'normal'\n",
        "        ))\n",
        "        previous_tokens.add(token)\n",
        "\n",
        "# התאמת המילים מבלי לחפוף אחת על השנייה\n",
        "adjust_text(texts)\n",
        "\n",
        "plt.title('Visualization of Tokens in Semantic Space (UMAP)', fontsize=26)\n",
        "plt.xlabel('UMAP Dimension 1', fontsize=20)\n",
        "plt.ylabel('UMAP Dimension 2', fontsize=20)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Gyn5Zfjv1IST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "\n",
        "def save_precomputed_data(indexes, vector_to_repo, all_vectors, all_tokens, embeddings, best_params):\n",
        "    \"\"\"\n",
        "    Save the relevant precomputed data to disk.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(\"/content/drive/MyDrive/GitHubRepositoriesProject/saved_data\"):\n",
        "        os.makedirs(\"/content/drive/MyDrive/GitHubRepositoriesProject/saved_data\")\n",
        "\n",
        "    # Save FAISS indexes\n",
        "    for nlist, index in indexes.items():\n",
        "        faiss.write_index(index, f\"/content/drive/MyDrive/GitHubRepositoriesProject/saved_data/faiss_index_{nlist}.index\")\n",
        "\n",
        "    # Save other data with pickle\n",
        "    data = {\n",
        "        \"vector_to_repo\": vector_to_repo,\n",
        "        \"all_vectors\": all_vectors,\n",
        "        \"all_tokens\": all_tokens,\n",
        "        \"embeddings_name\": embeddings.model.config.name_or_path,  # Save model name instead of the object\n",
        "        \"best_params\": best_params  # Save the best parameters found by Optuna\n",
        "    }\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/GitHubRepositoriesProject/saved_data/precomputed_data.pkl\", \"wb\") as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "\n",
        "\n",
        "save_precomputed_data(indexes, vector_to_repo, all_vectors, all_tokens, embeddings, best_params)\n",
        "indexes, vector_to_repo, all_vectors, all_tokens, embeddings, best_params = load_precomputed_data(nlist_values)\n"
      ],
      "metadata": {
        "id": "K7VL916q1tGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "--N5JmrP2aGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "qJbmsmc063e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6sVQ90iT_LeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results Analysis  \n",
        "#### Precision:  \n",
        "The results are very high (between **0.65 and 0.85**) – meaning most of the results retrieved by the system are relevant to the query.  \n",
        "\n",
        "#### Recall:  \n",
        "The results are **1.0** for all queries.  \n",
        "This means all relevant projects in the **Ground Truth** were successfully found by the system, which is excellent.  \n",
        "\n",
        "#### F1 Score:  \n",
        "The scores are very high (between **0.79 and 0.92**).  \n",
        "This shows an excellent balance between **Precision** and **Complete Retrieval (Recall)**."
      ],
      "metadata": {
        "id": "0fQIN8WYXDkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Pipeline using LangChain"
      ],
      "metadata": {
        "id": "0IfmLTajXLuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "def load_precomputed_data(nlist_values):\n",
        "    \"\"\"\n",
        "    Load the saved precomputed data from disk.\n",
        "    \"\"\"\n",
        "    indexes = {}\n",
        "\n",
        "    # Load FAISS indexes\n",
        "    for nlist in nlist_values:\n",
        "        index_path = f\"/content/drive/MyDrive/GitHubRepositoriesProject/saved_data/faiss_index_{nlist}.index\"\n",
        "        if os.path.exists(index_path):\n",
        "            index = faiss.read_index(index_path)\n",
        "            indexes[nlist] = index\n",
        "        else:\n",
        "            print(f\"Index file for nlist={nlist} not found at {index_path}.\")\n",
        "\n",
        "    # Load other data\n",
        "    data_path = \"/content/drive/MyDrive/GitHubRepositoriesProject/saved_data/precomputed_data.pkl\"\n",
        "    if os.path.exists(data_path):\n",
        "        with open(data_path, \"rb\") as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        vector_to_repo = data[\"vector_to_repo\"]\n",
        "        all_vectors = data[\"all_vectors\"]\n",
        "        all_tokens = data[\"all_tokens\"]\n",
        "        embeddings = CustomCodeBERTEmbeddings(model_name=data[\"embeddings_name\"])  # Reinitialize the embeddings object\n",
        "        best_params = data[\"best_params\"]\n",
        "\n",
        "        return indexes, vector_to_repo, all_vectors, all_tokens, embeddings, best_params\n",
        "    else:\n",
        "        print(f\"Precomputed data file not found at {data_path}.\")\n",
        "        return None, None, None, None, None, None"
      ],
      "metadata": {
        "id": "R72RkWpC_Kdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlist_values = [100, 200, 300, 500]\n",
        "\n",
        "indexes, vector_to_repo, all_vectors, all_tokens, embeddings, best_params = load_precomputed_data(nlist_values)"
      ],
      "metadata": {
        "id": "EFWK4rzv_M-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g2-CQAcqk--4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Improved RAG Pipeline ---\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from langchain.vectorstores import FAISS\n",
        "import faiss\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from google.colab import userdata\n",
        "from langchain_openai import OpenAI\n",
        "import os\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('open_ai_key')\n",
        "\n",
        "# ---- יצירת אובייקט ה-LLM ----\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# class CustomCodeBERTEmbeddings(Embeddings):\n",
        "#     def __init__(self, model_name=\"microsoft/codebert-base\"):\n",
        "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#         self.model = AutoModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "#     def embed_documents(self, texts, batch_size=64):\n",
        "#         embeddings = []\n",
        "#         for i in range(0, len(texts), batch_size):\n",
        "#             batch = texts[i:i + batch_size]\n",
        "#             inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "#             with torch.no_grad():\n",
        "#                 outputs = self.model(**inputs)\n",
        "#             batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "#             embeddings.extend(batch_embeddings)\n",
        "#         return np.array(embeddings)\n",
        "\n",
        "#     def embed_query(self, query):\n",
        "#         return self.embed_documents([query])[0]\n",
        "\n",
        "\n",
        "embeddings = CustomCodeBERTEmbeddings()\n",
        "\n",
        "\n",
        "def search_in_index(query, index, top_k=5, nprobe=10):\n",
        "    index.nprobe = nprobe\n",
        "    query_vector = embeddings.embed_query(query)\n",
        "    distances, indices = index.search(np.array([query_vector], dtype='float32'), top_k)\n",
        "    results = []\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        if idx == -1: continue\n",
        "        repo_index = vector_to_repo[idx]\n",
        "        repo_name = clean_df.iloc[repo_index]['Name']\n",
        "        repo_description = clean_df.iloc[repo_index]['Description']\n",
        "        repo_url = clean_df.iloc[repo_index]['URL']\n",
        "        repo_topics = clean_df.iloc[repo_index]['Topics']\n",
        "        score = distances[0][i]\n",
        "        results.append({\n",
        "            'name': repo_name,\n",
        "            'description': repo_description,\n",
        "            'url': repo_url,\n",
        "            'topics': repo_topics,\n",
        "            'score': score\n",
        "        })\n",
        "    return results\n",
        "\n",
        "best_nlist = 100\n",
        "index = indexes[best_nlist]\n",
        "\n",
        "def generate_answer_with_rag(query, top_k=5):\n",
        "    search_results = search_in_index(query, index, top_k)\n",
        "    if not search_results:\n",
        "        return \"No relevant repositories found.\"\n",
        "\n",
        "    prompt = (\n",
        "        f\"You are an expert in providing structured answers based on relevant GitHub repositories.\\n\"\n",
        "        f\"Provide a well-organized response for the query: '{query}'.\\n\\n\"\n",
        "        \"### Relevant Repositories:###\\n\"\n",
        "    )\n",
        "    for result in search_results:\n",
        "        prompt += (\n",
        "            f\" - Description: {result['description']}\\n\"\n",
        "            f\"- Repository: {result['name']}\\n\"\n",
        "            f\"  - Topics: {result['topics']}\\n\"\n",
        "            f\"  - URL: {result['url']}\\n\"\n",
        "            f\"  - Relevance Score: {result['score']}\\n\\n\"\n",
        "        )\n",
        "\n",
        "    response = llm(prompt)\n",
        "    return response\n",
        "\n"
      ],
      "metadata": {
        "id": "LfHIa4bSy9M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example Usage\n",
        "query = \"How to build deep learning models with PyTorch?\"\n",
        "answer = generate_answer_with_rag(query, top_k=5)\n"
      ],
      "metadata": {
        "id": "g0HtTSuLzBjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer.content)\n"
      ],
      "metadata": {
        "id": "d0SL_Zk1zSwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG evaluation"
      ],
      "metadata": {
        "id": "98IIWAaWEVpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install rouge_score"
      ],
      "metadata": {
        "id": "OFjQnzcYEGtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_search_results(optimized_results_df, ground_truth_df):\n",
        "    \"\"\"\n",
        "    Compare search results with the ground truth and calculate metrics (Precision, Recall, F1, ROUGE, BLEU, MRR).\n",
        "    \"\"\"\n",
        "    precision_list, recall_list, f1_list = [], [], []\n",
        "    rouge_scores, bleu_scores, mrr_scores = [], [], []\n",
        "    queries = optimized_results_df['query'].unique()\n",
        "\n",
        "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    for query in tqdm(queries, desc=\"Evaluating Queries\"):\n",
        "        relevant_projects = set(ground_truth_df[ground_truth_df['query'] == query]['repo_name'])\n",
        "        retrieved_projects = optimized_results_df[optimized_results_df['query'] == query]['repo_name'].tolist()\n",
        "\n",
        "        # Calculate Precision, Recall, F1\n",
        "        y_true = [1 if project in relevant_projects else 0 for project in retrieved_projects]\n",
        "        y_pred = [1] * len(retrieved_projects)\n",
        "\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "        f1_list.append(f1)\n",
        "\n",
        "        # Calculate ROUGE & BLEU for each query\n",
        "        relevant_texts = ground_truth_df[ground_truth_df['query'] == query]['repo_topics'].tolist()\n",
        "        retrieved_texts = optimized_results_df[optimized_results_df['query'] == query]['repo_topics'].tolist()\n",
        "\n",
        "        if retrieved_texts and relevant_texts:\n",
        "            # ROUGE Score Calculation\n",
        "            rouge_scores_per_query = []\n",
        "            for retrieved_text in retrieved_texts:\n",
        "                score = rouge_scorer_obj.score(relevant_texts[0], retrieved_text)\n",
        "                rouge_scores_per_query.append(score['rougeL'].fmeasure)\n",
        "\n",
        "            # BLEU Score Calculation (Taking average of top 5 results)\n",
        "            bleu_scores_per_query = [sentence_bleu([relevant_texts[0].split()], retrieved_text.split()) for retrieved_text in retrieved_texts[:5]]\n",
        "\n",
        "            rouge_scores.append(np.mean(rouge_scores_per_query))\n",
        "            bleu_scores.append(np.mean(bleu_scores_per_query))\n",
        "\n",
        "            # MRR Calculation\n",
        "            rank = 0\n",
        "            for i, retrieved_project in enumerate(retrieved_projects):\n",
        "                if retrieved_project in relevant_projects:\n",
        "                    rank = i + 1\n",
        "                    break\n",
        "            if rank > 0:\n",
        "                mrr_scores.append(1 / rank)\n",
        "            else:\n",
        "                mrr_scores.append(0)\n",
        "        else:\n",
        "            rouge_scores.append(0)\n",
        "            bleu_scores.append(0)\n",
        "            mrr_scores.append(0)\n",
        "\n",
        "    # Create Evaluation DataFrame\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'query': queries,\n",
        "        'precision': precision_list,\n",
        "        'recall': recall_list,\n",
        "        'f1': f1_list,\n",
        "        'ROUGE': rouge_scores,\n",
        "        'BLEU': bleu_scores,\n",
        "        'MRR': mrr_scores\n",
        "    })\n",
        "\n",
        "    # Displaying the results\n",
        "\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "metrics_df = evaluate_search_results(optimized_results_df, ground_truth_df)\n"
      ],
      "metadata": {
        "id": "rm_QuABQwGMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df"
      ],
      "metadata": {
        "id": "dHIjISFUERWt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}