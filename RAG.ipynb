{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMqQ7ttiMfPNWpDIWnP8xac",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noambassat/RAG_project/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain faiss-cpu transformers"
      ],
      "metadata": {
        "id": "QoiJXR-gztif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "NM3Wsm4ooN2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "id": "aMSdWm-0n8Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "id": "g4OVx76-lrhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install adjustText"
      ],
      "metadata": {
        "id": "y0K8IEW2z-6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install mlflow\n",
        "\n",
        "!pip install faiss\n",
        "\n",
        "!pip install transformers sentence-transformers\n",
        "\n",
        "!pip install gitpython\n"
      ],
      "metadata": {
        "id": "V7cm8iin-V4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from transformers import AutoTokenizer, AutoModel, AutoTokenizer\n",
        "from langchain.vectorstores import FAISS\n",
        "import faiss\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from google.colab import drive\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import json\n",
        "import random\n",
        "import umap\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from adjustText import adjust_text\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "from typing import List\n",
        "import mlflow\n",
        "from google.colab import userdata\n",
        "from transformers import AutoModel,\n",
        "\n",
        "path = \"/content/drive/MyDrive/GitHubRepositoriesProject/\"\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AzOqW6QXWhZW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d08dc382-c4b7-4c8f-c7c3-19cc60671d41"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df = pd.read_excel(\"/content/drive/MyDrive/GitHubRepositoriesProject/clean_df.xlsx\")"
      ],
      "metadata": {
        "id": "k9d86mKbjgWm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df.shape"
      ],
      "metadata": {
        "id": "MyHqlB3bcey7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0628d4aa-70e6-47f0-c3a8-5840bde20879"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11711, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "# pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "SjgipgROcFcX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the **CustomCodeBERTEmbeddings** Model  \n",
        "The **CodeBERT** model is designed for code-related data. It is based on **BERT** and trained with code data. This model allows generating representations of texts using a **tokenizer** and a **model**.  \n",
        "\n",
        "### The **embed_documents** Function  \n",
        "This function generates embeddings for each given text by:  \n",
        "- **Tokenization:** Breaking down the text into tokens.  \n",
        "- **Text Splitting:** Dividing texts based on length to fit the model’s input limit.  \n",
        "- **Batch Processing:** Producing embeddings in batches to improve efficiency."
      ],
      "metadata": {
        "id": "N-SA04qw1Ffh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df.dropna(subset = 'Topics',inplace = True)\n",
        "clean_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-bU6geJBwtu",
        "outputId": "f58ac85b-e6c2-46f0-de4a-59528ddda17e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11699, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "2yxpLsoh-9Wv",
        "outputId": "642878e8-6955-456b-99df-4cda704225d5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Name                                        Description  \\\n",
              "0              PyPOTS  toolboxlibrary data mining partially observed ...   \n",
              "1  changedetection.io  best simplest free open source website change ...   \n",
              "\n",
              "                                              URL          Created At  \\\n",
              "0              https://github.com/WenjieDu/PyPOTS 2022-03-29 14:22:47   \n",
              "1  https://github.com/dgtlmoon/changedetection.io 2021-01-27 16:03:30   \n",
              "\n",
              "           Updated At  Size  Stars  \\\n",
              "0 2023-09-25 04:20:18  7812    438   \n",
              "1 2023-09-21 11:00:40  6801  11908   \n",
              "\n",
              "                                              Topics  Overlap_Score  \\\n",
              "0  classification, clustering, data mining, forec...             37   \n",
              "1  back in stock, change alert, change detection,...             27   \n",
              "\n",
              "                                        Common_Words  Year  \n",
              "0  ['data', 'forecasting', 'incomplete', 'irregul...  2022  \n",
              "1  ['change', 'defacement', 'detection', 'monitor...  2021  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b23218be-b319-4240-8a3d-21f2505d3727\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Description</th>\n",
              "      <th>URL</th>\n",
              "      <th>Created At</th>\n",
              "      <th>Updated At</th>\n",
              "      <th>Size</th>\n",
              "      <th>Stars</th>\n",
              "      <th>Topics</th>\n",
              "      <th>Overlap_Score</th>\n",
              "      <th>Common_Words</th>\n",
              "      <th>Year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PyPOTS</td>\n",
              "      <td>toolboxlibrary data mining partially observed ...</td>\n",
              "      <td>https://github.com/WenjieDu/PyPOTS</td>\n",
              "      <td>2022-03-29 14:22:47</td>\n",
              "      <td>2023-09-25 04:20:18</td>\n",
              "      <td>7812</td>\n",
              "      <td>438</td>\n",
              "      <td>classification, clustering, data mining, forec...</td>\n",
              "      <td>37</td>\n",
              "      <td>['data', 'forecasting', 'incomplete', 'irregul...</td>\n",
              "      <td>2022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>changedetection.io</td>\n",
              "      <td>best simplest free open source website change ...</td>\n",
              "      <td>https://github.com/dgtlmoon/changedetection.io</td>\n",
              "      <td>2021-01-27 16:03:30</td>\n",
              "      <td>2023-09-21 11:00:40</td>\n",
              "      <td>6801</td>\n",
              "      <td>11908</td>\n",
              "      <td>back in stock, change alert, change detection,...</td>\n",
              "      <td>27</td>\n",
              "      <td>['change', 'defacement', 'detection', 'monitor...</td>\n",
              "      <td>2021</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b23218be-b319-4240-8a3d-21f2505d3727')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b23218be-b319-4240-8a3d-21f2505d3727 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b23218be-b319-4240-8a3d-21f2505d3727');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4c520aba-cee3-4b9d-9b3d-95aa0b3222ec\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4c520aba-cee3-4b9d-9b3d-95aa0b3222ec')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4c520aba-cee3-4b9d-9b3d-95aa0b3222ec button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "clean_df",
              "summary": "{\n  \"name\": \"clean_df\",\n  \"rows\": 11699,\n  \"fields\": [\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11588,\n        \"samples\": [\n          \"beartype\",\n          \"hvcc\",\n          \"DeepTrade\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11610,\n        \"samples\": [\n          \"used deep automatic portrait matting pytorth\",\n          \"run evaluation llms human eval benchmark\",\n          \"mastering atari discrete world models\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11699,\n        \"samples\": [\n          \"https://github.com/fishaudio/fish-diffusion\",\n          \"https://github.com/shawnwun/NNDIAL\",\n          \"https://github.com/mniepert/mmkb\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Created At\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2017-01-01 04:20:54\",\n        \"max\": \"2023-09-24 11:21:54\",\n        \"num_unique_values\": 11697,\n        \"samples\": [\n          \"2018-08-17 20:23:29\",\n          \"2020-01-25 12:47:02\",\n          \"2020-03-17 15:54:27\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Updated At\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2022-02-19 18:27:20\",\n        \"max\": \"2023-09-25 19:23:47\",\n        \"num_unique_values\": 11566,\n        \"samples\": [\n          \"2023-09-20 11:07:48\",\n          \"2023-09-24 12:26:16\",\n          \"2023-09-16 21:04:14\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 931585,\n        \"min\": 0,\n        \"max\": 93317358,\n        \"num_unique_values\": 7824,\n        \"samples\": [\n          9069,\n          79923,\n          879\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Stars\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3505,\n        \"min\": 167,\n        \"max\": 229569,\n        \"num_unique_values\": 2447,\n        \"samples\": [\n          12982,\n          522,\n          2059\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Topics\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11554,\n        \"samples\": [\n          \"autoencoder, computer vision, deep learning, paper implementations, pytorch, representation learning, representations learning, slam, variational autoencoder, vision\",\n          \"artificial intelligence, attention mechanisms, audio synthesis, deep learning, transformers\",\n          \"digit, image processing, mathematics, pil, pillow, primality tests, prime numbers, sympy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Overlap_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 37,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          18,\n          13,\n          17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Common_Words\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8413,\n        \"samples\": [\n          \"['airflow', 'apache', 'dbt']\",\n          \"['data', 'extension', 'qlik', 'science', 'server']\",\n          \"['synthesis']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 2017,\n        \"max\": 2023,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          2022,\n          2021,\n          2020\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df[\"Topics\"].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "gP5WNQr0_XgT",
        "outputId": "067b1a6c-6541-4ba1-ec6a-a2f954a321ad"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    classification, clustering, data mining, forec...\n",
              "1    back in stock, change alert, change detection,...\n",
              "2    aco, ant colony optimization, artificial bee c...\n",
              "3    instagram, instagram api, instagram bot, insta...\n",
              "4    dashboard, log analysis, log parsing, scrapy, ...\n",
              "Name: Topics, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>classification, clustering, data mining, forec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>back in stock, change alert, change detection,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>aco, ant colony optimization, artificial bee c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>instagram, instagram api, instagram bot, insta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dashboard, log analysis, log parsing, scrapy, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the Use Case  \n",
        "The use case involves **highly specific technical categories** (algorithm names, library names, tool descriptions, etc.).  \n",
        "\n",
        "### Why Simple Semantic Search Is Not Enough  \n",
        "- Simple embeddings may not accurately capture the precise meaning of technical terms.  \n",
        "- A model that understands technical terminology in depth is required (e.g., **CodeBERT**).  \n",
        "\n",
        "### Evaluation Approach  \n",
        "- Multiple indexes need to be evaluated to determine the most effective one.  \n",
        "- Using **MLFlow** will be ideal for tracking experiments and comparing results.  \n",
        "\n",
        "The plan is to build, test, and compare different index configurations to identify the most suitable setup for technical term retrieval.  \n",
        "\n",
        "Would you like me to proceed with the implementation of this approach?"
      ],
      "metadata": {
        "id": "dnsTQnU6AQZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling"
      ],
      "metadata": {
        "id": "JxM73IcBDdGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def radom_samplig(df):\n",
        "  return df.sample(frac=0.1, random_state=42)\n",
        "\n",
        "sample_df = radom_samplig(clean_df)\n",
        "sample_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9S5RMzmDfBA",
        "outputId": "3afe6f81-5daf-4fb6-c18f-853f79527e2d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1170, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "FwhtAiBxAzuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Models Overview  \n",
        "\n",
        "1. **CodeBERT**  \n",
        "   - Transformer-based model developed by Microsoft, trained on code-documentation pairs from public repositories.  \n",
        "   - **Strengths:** Excellent understanding of technical terms and code, ideal for topics related to programming languages, libraries, and algorithms.  \n",
        "   - **Source:** microsoft/codebert-base  \n",
        "\n",
        "2. **SciBERT**  \n",
        "   - Model by Allen Institute for AI, trained on scientific papers in computer science and biology.  \n",
        "   - **Strengths:** Excels in understanding technical and scientific texts, suitable for topics with academic or research-related terminology.  \n",
        "   - **Source:** allenai/scibert_scivocab_uncased  \n",
        "\n",
        "3. **OpenAI's GPT Embeddings (text-embedding-ada-002)**  \n",
        "   - Latest generation embedding model by OpenAI for high-quality general-purpose text embeddings.  \n",
        "   - **Strengths:** Provides deep semantic understanding, suitable for various topics but requires access to OpenAI's API.  \n",
        "   - **Source:** OpenAI API  "
      ],
      "metadata": {
        "id": "mkwW_fl_A5HP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "hNShvRVgWLXR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_model_and_tokenizer(model_name, device):\n",
        "    \"\"\" Load model and tokenizer from HuggingFace. \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name).to(device)\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_embeddings_for_texts(texts, model, tokenizer, device):\n",
        "    \"\"\" Generate embeddings for a list of texts. \"\"\"\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            embedding = model(**inputs).last_hidden_state.mean(dim=1)  # Mean Pooling\n",
        "        embeddings.append(embedding.cpu().numpy().flatten())\n",
        "    return embeddings\n",
        "\n",
        "def generate_embeddings(df, embedding_type, batch_size=100, device=device, openai_api_key=None):\n",
        "\n",
        "    if openai_api_key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key  # Set OpenAI API key if provided\n",
        "\n",
        "    # Load the correct embedding model\n",
        "    if embedding_type == \"GPT\":\n",
        "        embedding_model = OpenAIEmbeddings(model = \"text-embedding-ada-002\")\n",
        "    else:\n",
        "        model_name = \"microsoft/codebert-base\" if embedding_type == \"CodeBERT\" else \"allenai/scibert_scivocab_uncased\"\n",
        "        model, tokenizer = load_model_and_tokenizer(model_name, device)\n",
        "\n",
        "    embedding_col = f\"embedding_type_{embedding_type}\"\n",
        "\n",
        "    with mlflow.start_run(run_name=f\"{embedding_type}_Embedding_Extraction\"):\n",
        "        mlflow.log_param(\"embedding_type\", embedding_type)\n",
        "\n",
        "        embeddings = []\n",
        "        topics_list = []\n",
        "\n",
        "        for topic in df[\"Topics\"]:\n",
        "            topics = \" \".join(topic) if isinstance(topic, list) else topic\n",
        "            topics_list.append(topics)\n",
        "\n",
        "        # Process in batches with tqdm\n",
        "        for i in tqdm(range(0, len(topics_list), batch_size), desc=f\"Generating Embeddings for {embedding_type}\", ncols=100):\n",
        "            batch = topics_list[i:i + batch_size]\n",
        "\n",
        "            if embedding_type == \"GPT\":\n",
        "                batch_embeddings = embedding_model.embed_documents(batch)\n",
        "            else:\n",
        "                batch_embeddings = generate_embeddings_for_texts(batch, model, tokenizer, device)\n",
        "\n",
        "            embeddings.extend(batch_embeddings)\n",
        "\n",
        "        mlflow.log_metric(\"num_documents\", len(embeddings))\n",
        "        df[embedding_col] = embeddings\n",
        "\n",
        "\n",
        "        output_file = f\"{embedding_type}_embeddings.xlsx\"\n",
        "        df.to_excel(output_file, index=False)\n",
        "        mlflow.log_artifact(output_file)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "qTEwEKfKA2MM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for embedding_type in [\"CodeBERT\", \"SciBERT\", \"GPT\"]:\n",
        "    sample_df = generate_embeddings(sample_df, embedding_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdgCJKWfASDo",
        "outputId": "3b3ad37b-21d4-4f83-f906-3d1155bf6767"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Embeddings for CodeBERT: 100%|███████████████████████████| 12/12 [00:09<00:00,  1.23it/s]\n",
            "Generating Embeddings for SciBERT:   0%|                                     | 0/12 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Generating Embeddings for SciBERT: 100%|████████████████████████████| 12/12 [00:09<00:00,  1.27it/s]\n",
            "Generating Embeddings for GPT: 100%|████████████████████████████████| 12/12 [00:09<00:00,  1.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "cI0JkI12PE2x",
        "outputId": "d5be1284-a86b-48ed-c8e6-184df8331b9f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Name                                        Description  \\\n",
              "11513  fish-diffusion              easy understand tts svs svc framework   \n",
              "2087           NNDIAL  nndial open source toolkit building end end tr...   \n",
              "11323            mmkb                                data modalities kbs   \n",
              "\n",
              "                                               URL          Created At  \\\n",
              "11513  https://github.com/fishaudio/fish-diffusion 2023-01-09 11:01:24   \n",
              "2087            https://github.com/shawnwun/NNDIAL 2017-05-26 14:38:00   \n",
              "11323             https://github.com/mniepert/mmkb 2018-02-21 12:33:24   \n",
              "\n",
              "               Updated At   Size  Stars  \\\n",
              "11513 2023-09-24 01:44:44  64316    458   \n",
              "2087  2023-09-01 08:40:42  73976    346   \n",
              "11323 2023-09-18 04:12:17  34709    314   \n",
              "\n",
              "                                                  Topics  Overlap_Score  \\\n",
              "11513                 diffusion, pytorch, soundgenerator              0   \n",
              "2087   dialogue, dialogue agents, dialogue generation...              6   \n",
              "11323                          freebase, knowledge graph              0   \n",
              "\n",
              "                  Common_Words  Year  \\\n",
              "11513                       []  2023   \n",
              "2087   ['dialogue', 'systems']  2017   \n",
              "11323                       []  2018   \n",
              "\n",
              "                                 embedding_type_CodeBERT  \\\n",
              "11513  [-0.17904958, 0.025005545, 0.43965673, 0.22487...   \n",
              "2087   [-0.23475327, 0.05144652, 0.24216096, 0.262912...   \n",
              "11323  [-0.035101186, 0.24884412, -0.0036410564, 0.21...   \n",
              "\n",
              "                                  embedding_type_SciBERT  \\\n",
              "11513  [0.053714093, 0.085433334, 0.29141527, 1.08931...   \n",
              "2087   [0.24996585, 0.52364385, 1.1472524, 0.16191079...   \n",
              "11323  [-0.37098655, 0.6352393, 0.012849961, 0.187038...   \n",
              "\n",
              "                                      embedding_type_GPT  \n",
              "11513  [-0.022400148187879415, -0.010923015872849513,...  \n",
              "2087   [-0.019865424245987043, -0.004319982778261262,...  \n",
              "11323  [0.021688691414892367, -0.007480273659060247, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9f7a444d-d857-478d-a227-7a59cd93b3c1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Description</th>\n",
              "      <th>URL</th>\n",
              "      <th>Created At</th>\n",
              "      <th>Updated At</th>\n",
              "      <th>Size</th>\n",
              "      <th>Stars</th>\n",
              "      <th>Topics</th>\n",
              "      <th>Overlap_Score</th>\n",
              "      <th>Common_Words</th>\n",
              "      <th>Year</th>\n",
              "      <th>embedding_type_CodeBERT</th>\n",
              "      <th>embedding_type_SciBERT</th>\n",
              "      <th>embedding_type_GPT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11513</th>\n",
              "      <td>fish-diffusion</td>\n",
              "      <td>easy understand tts svs svc framework</td>\n",
              "      <td>https://github.com/fishaudio/fish-diffusion</td>\n",
              "      <td>2023-01-09 11:01:24</td>\n",
              "      <td>2023-09-24 01:44:44</td>\n",
              "      <td>64316</td>\n",
              "      <td>458</td>\n",
              "      <td>diffusion, pytorch, soundgenerator</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>2023</td>\n",
              "      <td>[-0.17904958, 0.025005545, 0.43965673, 0.22487...</td>\n",
              "      <td>[0.053714093, 0.085433334, 0.29141527, 1.08931...</td>\n",
              "      <td>[-0.022400148187879415, -0.010923015872849513,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2087</th>\n",
              "      <td>NNDIAL</td>\n",
              "      <td>nndial open source toolkit building end end tr...</td>\n",
              "      <td>https://github.com/shawnwun/NNDIAL</td>\n",
              "      <td>2017-05-26 14:38:00</td>\n",
              "      <td>2023-09-01 08:40:42</td>\n",
              "      <td>73976</td>\n",
              "      <td>346</td>\n",
              "      <td>dialogue, dialogue agents, dialogue generation...</td>\n",
              "      <td>6</td>\n",
              "      <td>['dialogue', 'systems']</td>\n",
              "      <td>2017</td>\n",
              "      <td>[-0.23475327, 0.05144652, 0.24216096, 0.262912...</td>\n",
              "      <td>[0.24996585, 0.52364385, 1.1472524, 0.16191079...</td>\n",
              "      <td>[-0.019865424245987043, -0.004319982778261262,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11323</th>\n",
              "      <td>mmkb</td>\n",
              "      <td>data modalities kbs</td>\n",
              "      <td>https://github.com/mniepert/mmkb</td>\n",
              "      <td>2018-02-21 12:33:24</td>\n",
              "      <td>2023-09-18 04:12:17</td>\n",
              "      <td>34709</td>\n",
              "      <td>314</td>\n",
              "      <td>freebase, knowledge graph</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>2018</td>\n",
              "      <td>[-0.035101186, 0.24884412, -0.0036410564, 0.21...</td>\n",
              "      <td>[-0.37098655, 0.6352393, 0.012849961, 0.187038...</td>\n",
              "      <td>[0.021688691414892367, -0.007480273659060247, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f7a444d-d857-478d-a227-7a59cd93b3c1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9f7a444d-d857-478d-a227-7a59cd93b3c1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9f7a444d-d857-478d-a227-7a59cd93b3c1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-06063849-2c2b-4c33-acf6-ade2308407df\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-06063849-2c2b-4c33-acf6-ade2308407df')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-06063849-2c2b-4c33-acf6-ade2308407df button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "sample_df",
              "summary": "{\n  \"name\": \"sample_df\",\n  \"rows\": 1170,\n  \"fields\": [\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1166,\n        \"samples\": [\n          \"FCOS\",\n          \"awesome-dash\",\n          \"seglearn\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1163,\n        \"samples\": [\n          \"realtime web apps dashboards r\",\n          \"defence metric learning speaker recognition\",\n          \"derived climate climate xarray\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1170,\n        \"samples\": [\n          \"https://github.com/KaiyangZhou/pytorch-vsumm-reinforce\",\n          \"https://github.com/Raptor123471/DingoLingo\",\n          \"https://github.com/lewangdev/youtube-drive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Created At\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2017-01-04 00:25:05\",\n        \"max\": \"2023-09-20 15:19:38\",\n        \"num_unique_values\": 1170,\n        \"samples\": [\n          \"2018-04-19 16:36:11\",\n          \"2020-08-12 03:33:20\",\n          \"2022-05-26 15:47:32\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Updated At\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2023-02-12 01:51:31\",\n        \"max\": \"2023-09-25 18:17:38\",\n        \"num_unique_values\": 1167,\n        \"samples\": [\n          \"2023-09-15 21:50:04\",\n          \"2023-09-20 02:34:52\",\n          \"2023-09-25 07:33:28\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2831292,\n        \"min\": 4,\n        \"max\": 93317358,\n        \"num_unique_values\": 1069,\n        \"samples\": [\n          19,\n          892,\n          1609\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Stars\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2537,\n        \"min\": 167,\n        \"max\": 36446,\n        \"num_unique_values\": 668,\n        \"samples\": [\n          5139,\n          342,\n          1064\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Topics\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1168,\n        \"samples\": [\n          \"bbc news, cnn, fox news, google news, newsapi, top headlines news\",\n          \"chatbot, deep learning, deeplearning, korean, korean chatbot, sentence classification, sequance tagging, web crawler\",\n          \"bugbounty, mobile security, reverse engineering, ssl pinning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Overlap_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 37,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          0,\n          13,\n          20\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Common_Words\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 970,\n        \"samples\": [\n          \"['performance']\",\n          \"['imessage']\",\n          \"['engine', 'search']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 2017,\n        \"max\": 2023,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          2023,\n          2017,\n          2021\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding_type_CodeBERT\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding_type_SciBERT\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding_type_GPT\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Index vactors"
      ],
      "metadata": {
        "id": "YJ8iYdmTRwco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **IndexFlatL2**  \n",
        "Description: A simple index that performs exact searches by calculating Euclidean distance (L2) between vectors.  \n",
        "Advantages: Provides fully accurate results.  \n",
        "Disadvantages: Slow and inefficient for large datasets.  \n",
        "Recommended Use: Best for small datasets when accuracy is the top priority.  \n",
        "\n",
        "2. **IndexIVFPQ (Inverted File with Product Quantization)**  \n",
        "Description: Combines inverted files with product quantization to reduce computations and speed up searches.  \n",
        "Advantages: Fast searches on large datasets with reduced memory usage.  \n",
        "Disadvantages: Requires training; accuracy depends on parameters like nlist (number of clusters) and nprobe (number of probes).  \n",
        "Recommended Use: Suitable for large datasets where speed is important, with some accuracy compromise.  \n",
        "\n",
        "3. **IndexHNSW (Hierarchical Navigable Small World)**  \n",
        "Description: Graph-based index enabling efficient approximate searches with sub-linear time complexity.  \n",
        "Advantages: Good balance between speed and accuracy; ideal for approximate searches.  \n",
        "Disadvantages: High memory consumption; doesn't support adding vectors with custom IDs.  \n",
        "Recommended Use: Best when search speed is crucial, even if it slightly affects accuracy.  "
      ],
      "metadata": {
        "id": "fM6yq6_2R0O5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ts3bwINlR07O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TTPBWVt5ASGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "URN-tV50ASIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EEBTqqfyASKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jYqVVOqcASNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3MzjB7udASPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6iwzmPGTRzv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hETq4eN6ASRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3nFf8o8JAST5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uNyOg2HWASV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nNehLNvpASYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fnpw5Ss_ASal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gu_ySOInAScx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7EIgO35lASfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZNfpnYZAShc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8jqbI_LfASjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCodeBERTEmbeddings(Embeddings):\n",
        "    def __init__(self, model_name=\"microsoft/codebert-base\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "    def embed_documents(self, texts, batch_size=64):  # הוספנו batch_size כארגומנט\n",
        "        embeddings = []\n",
        "\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i + batch_size]\n",
        "            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # שימוש ב-CLS Token\n",
        "            embeddings.extend(batch_embeddings)\n",
        "\n",
        "        return np.array(embeddings)\n",
        "\n",
        "    def embed_query(self, query):\n",
        "        return self.embed_documents([query])[0]\n",
        "\n",
        "embeddings = CustomCodeBERTEmbeddings()\n",
        "\n"
      ],
      "metadata": {
        "id": "_nR5tq08WUlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation  \n",
        "At this stage, the program prepares **all_tokens** and **index_mapping**, allowing me to track which tokens belong to each position in the data.  \n",
        "\n",
        "Then, the **embed_documents** function is used to generate embeddings for the tokens. Each embedding is stored as a vector, and the information is saved in a dictionary called **vector_to_repo** to link each embedding to its original location in the data."
      ],
      "metadata": {
        "id": "A0l4n2zX1QbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_vectors, vector_to_repo = [], {}\n",
        "current_index = 0\n",
        "batch_size = 64\n",
        "\n",
        "# הכנת רשימות שמכילות את כל ה-Tokens מתוך ה-DataFrame\n",
        "all_tokens, index_mapping = [], []\n",
        "for index, topic in tqdm(enumerate(clean_df[\"Topics\"]), total=len(clean_df), desc=\"Extracting Tokens\"):\n",
        "    if not(isinstance(topic, str) and len(topic.strip()) > 0): continue\n",
        "    tokens = [token.strip() for token in topic.split(\",\") if token.strip()]\n",
        "    all_tokens.extend(tokens)\n",
        "    index_mapping.extend([index] * len(tokens))\n",
        "\n",
        "# יצירת האימבדינגס בצורה יעילה עם Batching\n",
        "all_vectors = embeddings.embed_documents(all_tokens, batch_size=batch_size)\n",
        "\n",
        "# שמירת המידע במילון כדי שנוכל לחזור למיקום המקורי בדאטה\n",
        "for i, vector in enumerate(all_vectors):\n",
        "    vector_to_repo[i] = index_mapping[i]\n",
        "\n",
        "# המרת הוקטורים למערך NumPy\n",
        "all_vectors = np.array(all_vectors, dtype='float32')\n"
      ],
      "metadata": {
        "id": "YNBu7zWDYNA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Indexes with FAISS  \n",
        "**FAISS** is a tool optimized for vector-based searches. At this stage, the system creates an **Index** for each different **nlist** group.  \n",
        "\n",
        "Different indexes are built to allow efficient storage and searching over all the generated embeddings.  \n",
        "\n",
        "A new index is constructed for each **nlist**, which is periodically evaluated through searches to select the optimal values."
      ],
      "metadata": {
        "id": "q397P6sl1VUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_optimized_faiss_indexes(all_vectors, nlist_values):\n",
        "    \"\"\"\n",
        "    Creating Different Indexes with FAISS for Various nlist Values\n",
        "    \"\"\"\n",
        "    d = all_vectors.shape[1]\n",
        "    indexes = {}\n",
        "\n",
        "    for nlist in nlist_values:\n",
        "        quantizer = faiss.IndexFlatIP(d)\n",
        "        index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\n",
        "\n",
        "        index.train(all_vectors)\n",
        "        index.add(all_vectors)\n",
        "\n",
        "        indexes[nlist] = index\n",
        "        # print(f\"\\n Created FAISS index with nlist={nlist}. Number of embeddings indexed: {index.ntotal}\")\n",
        "\n",
        "    return indexes\n",
        "\n",
        "\n",
        "nlist_values = [100, 200, 300, 500]\n",
        "indexes = create_optimized_faiss_indexes(all_vectors, nlist_values)\n"
      ],
      "metadata": {
        "id": "hyP03PpkcUMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Searching with FAISS  \n",
        "At this stage, the search is performed within the created indexes. A user query is provided (e.g., \"deep learning\"), and the model searches for the most similar results by comparing embeddings.  \n",
        "\n",
        "The **search_in_index** function searches for the closest words to the query and returns the results along with the words, topics, and relevant links."
      ],
      "metadata": {
        "id": "L5b3EjRz1eKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def search_in_index(query: str, index, top_k: int = 10, nprobe: int = 10):\n",
        "#     \"\"\"\n",
        "#     Advanced Search with FAISS using IndexIVFFlat\n",
        "#     \"\"\"\n",
        "#     index.nprobe = nprobe\n",
        "#     query_vector = embeddings.embed_query(query)\n",
        "\n",
        "#     start_time = time.time()  # מדידת זמן ריצה\n",
        "#     distances, indices = index.search(np.array([query_vector], dtype='float32'), top_k)\n",
        "#     end_time = time.time()\n",
        "\n",
        "#     results = []\n",
        "#     for i, idx in enumerate(indices[0]):\n",
        "#         if idx == -1:\n",
        "#             continue\n",
        "\n",
        "#         repo_index = vector_to_repo[idx]\n",
        "#         repo_name = clean_df.iloc[repo_index][\"Name\"]\n",
        "#         repo_url = clean_df.iloc[repo_index][\"URL\"]\n",
        "#         repo_topics = clean_df.iloc[repo_index][\"Topics\"]\n",
        "#         score = distances[0][i]\n",
        "\n",
        "#         results.append({\n",
        "#             \"name\": repo_name,\n",
        "#             \"url\": repo_url,\n",
        "#             \"topics\": repo_topics,\n",
        "#             \"score\": score\n",
        "#         })\n",
        "\n",
        "#     search_time = end_time - start_time\n",
        "#     return results, search_time\n"
      ],
      "metadata": {
        "id": "6sCmP0jOYpsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid Search  \n",
        "To maximize system performance, a **Grid Search** is performed where different parameter values, such as **nlist** and **nprobe**, are tested.  \n",
        "\n",
        "This process allows me to evaluate search time, the number of relevant results, and filter the results optimally."
      ],
      "metadata": {
        "id": "rpyAN_nC1mpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def grid_search_faiss(queries, indexes, nprobe_values, top_k=10):\n",
        "#     \"\"\"\n",
        "#     Performing Searches for All Combinations of nlist and nprobe\n",
        "#     \"\"\"\n",
        "#     results_summary = []\n",
        "\n",
        "#     for nlist, index in indexes.items():\n",
        "#         for nprobe in nprobe_values:\n",
        "#             for query in queries:\n",
        "#                 search_results, search_time = search_in_index(query, index, top_k=top_k, nprobe=nprobe)\n",
        "\n",
        "\n",
        "#                 found_projects = [result['name'] for result in search_results]\n",
        "#                 found_topics = [result['topics'] for result in search_results]\n",
        "#                 found_urls = [result['url'] for result in search_results]\n",
        "#                 found_scores = [result['score'] for result in search_results]\n",
        "\n",
        "\n",
        "#                 results_summary.append({\n",
        "#                     \"query\": query,\n",
        "#                     \"nlist\": nlist,\n",
        "#                     \"nprobe\": nprobe,\n",
        "#                     \"top_k\": top_k,\n",
        "#                     \"search_time\": search_time,\n",
        "#                     \"relevant_results\": len(found_projects),\n",
        "#                     \"found_projects\": found_projects,\n",
        "#                     \"found_topics\": found_topics,\n",
        "#                     \"found_urls\": found_urls,\n",
        "#                     \"found_scores\": found_scores\n",
        "\n",
        "#                 })\n",
        "\n",
        "\n",
        "#     results_df = pd.DataFrame(results_summary)\n",
        "#     return results_df\n"
      ],
      "metadata": {
        "id": "cOOQkGfzpmsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# queries = [\"deep learning\", \"neural networks\", \"python libraries\", \"computer vision\", \"natural language processing\"]\n",
        "# nprobe_values = [10, 20, 30, 50]\n",
        "\n",
        "# results_df = grid_search_faiss(queries, indexes, nprobe_values, top_k=10)"
      ],
      "metadata": {
        "id": "CJg6Wqyypocf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# results_df.T"
      ],
      "metadata": {
        "id": "1DIjb2QuquhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# results_df.to_excel(\"/content/drive/MyDrive/GitHubRepositoriesProject/results_df.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "LXtv-IOQ3Xam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "81wyNNH7pmYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "path = \"/content/drive/MyDrive/GitHubRepositoriesProject/\"\n",
        "\n",
        "def save_parameters(all_vectors, vector_to_repo, all_tokens, indexes, embeddings):\n",
        "    \"\"\"\n",
        "    Save all the required data to disk.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path + \"saved_data\"):\n",
        "        os.makedirs(path + \"saved_data\")\n",
        "\n",
        "    # Save embeddings and vector_to_repo\n",
        "    with open(path + \"saved_data/embeddings_data.pkl\", \"wb\") as f:\n",
        "        pickle.dump({\n",
        "            \"all_vectors\": all_vectors,\n",
        "            \"vector_to_repo\": vector_to_repo,\n",
        "            \"all_tokens\": all_tokens,\n",
        "            \"embeddings_name\": embeddings.model.config.name_or_path  # Save model name instead of the object\n",
        "        }, f)\n",
        "\n",
        "    # Save FAISS indexes\n",
        "    for nlist, index in indexes.items():\n",
        "        faiss.write_index(index, path + f\"saved_data/faiss_index_{nlist}.index\")\n",
        "\n",
        "    print(\"All data has been successfully saved.\")\n",
        "\n",
        "save_parameters(all_vectors, vector_to_repo, all_tokens, indexes, embeddings)\n"
      ],
      "metadata": {
        "id": "l6xOlwnHHy-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering\n"
      ],
      "metadata": {
        "id": "ZzUvLmrKwFUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MiniBatch KNN"
      ],
      "metadata": {
        "id": "kvMoWCO3wLLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from adjustText import adjust_text\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# ---- 1. הפחתת ממדים בעזרת UMAP ----\n",
        "def reduce_with_umap(all_vectors, n_neighbors=30, min_dist=0.1, n_components=2):\n",
        "    umap_reducer = umap.UMAP(\n",
        "        n_neighbors=n_neighbors,\n",
        "        min_dist=min_dist,\n",
        "        n_components=n_components,\n",
        "        metric='cosine',\n",
        "        random_state=42\n",
        "    )\n",
        "    reduced_embeddings = umap_reducer.fit_transform(all_vectors)\n",
        "    return reduced_embeddings"
      ],
      "metadata": {
        "id": "Y56ryD4QwIFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ---- 2. ביצוע MiniBatch KMeans ----\n",
        "def run_mini_batch_kmeans(reduced_embeddings, n_clusters=20):\n",
        "    kmeans = MiniBatchKMeans(\n",
        "        n_clusters=n_clusters,\n",
        "        batch_size=1000,\n",
        "        random_state=42,\n",
        "        max_iter=300\n",
        "    )\n",
        "    labels = kmeans.fit_predict(reduced_embeddings)\n",
        "    return labels, kmeans\n",
        "\n",
        "# ---- 3. הצגת תוצאות על גרף ----\n",
        "def plot_clusters_with_words(reduced_embeddings, labels, all_tokens, title='MiniBatch KMeans Clustering'):\n",
        "    embedding_df = pd.DataFrame({\n",
        "        'UMAP1': reduced_embeddings[:, 0],\n",
        "        'UMAP2': reduced_embeddings[:, 1],\n",
        "        'Token': all_tokens,\n",
        "        'Cluster': labels\n",
        "    })\n",
        "\n",
        "    # ---- חישוב תדירות מילות מפתח ----\n",
        "    token_frequency = embedding_df['Token'].value_counts()\n",
        "    embedding_df['Frequency'] = embedding_df['Token'].map(token_frequency)\n",
        "\n",
        "    # ---- סינון מונחים לפי שכיחותם ----\n",
        "    threshold_frequency = 5  # הצגת מונחים שמופיעים לפחות מספר פעמים זה\n",
        "    filtered_df = embedding_df[embedding_df['Frequency'] >= threshold_frequency]\n",
        "\n",
        "    # ---- נירמול התדירות בין 0 ל-1 ----\n",
        "    scaler = MinMaxScaler()\n",
        "    embedding_df['Frequency_Scaled'] = scaler.fit_transform(embedding_df[['Frequency']])\n",
        "\n",
        "    # ---- הצגת גרף ----\n",
        "    plt.figure(figsize=(18, 12))\n",
        "    sns.scatterplot(\n",
        "        data=embedding_df,\n",
        "        x='UMAP1', y='UMAP2',\n",
        "        hue='Cluster',\n",
        "        palette='tab20',\n",
        "        legend='full',\n",
        "        alpha=0.6\n",
        "    )\n",
        "\n",
        "    previous_tokens = set()  # רשימה לשמירה על המילים שכבר הוצגו\n",
        "    texts = []  # רשימה של הטקסטים שמוסיפים לגרף\n",
        "\n",
        "    # הוספת טקסטים למרכזי קלאסטרים\n",
        "    for cluster_label in embedding_df['Cluster'].unique():\n",
        "        cluster_df = filtered_df[filtered_df['Cluster'] == cluster_label]\n",
        "\n",
        "        # סידור לפי תדירות המילים בתוך כל קלאסטר\n",
        "        top_words = cluster_df.groupby('Token')['Frequency'].sum().sort_values(ascending=False)\n",
        "\n",
        "        # בחירת מילה הכי שכיחה שאינה חופפת למילים שכבר הודפסו\n",
        "        for token, freq in top_words.items():\n",
        "            cluster_center = cluster_df[['UMAP1', 'UMAP2']].mean()\n",
        "            if token not in previous_tokens:\n",
        "                texts.append(plt.text(\n",
        "                    cluster_center['UMAP1'],\n",
        "                    cluster_center['UMAP2'],\n",
        "                    token,\n",
        "                    fontsize=min(20, 8 + freq / 2),\n",
        "                    alpha=0.9,\n",
        "                    weight='bold' if freq > 50 else 'normal'\n",
        "                ))\n",
        "                previous_tokens.add(token)\n",
        "                break  # לקחת רק את המילה הכי שכיחה לקלאסטר\n",
        "\n",
        "    # התאמת המילים מבלי לחפוף אחת על השנייה\n",
        "    adjust_text(texts)\n",
        "\n",
        "    plt.title(title, fontsize=22)\n",
        "    plt.xlabel('UMAP Dimension 1', fontsize=16)\n",
        "    plt.ylabel('UMAP Dimension 2', fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "    return embedding_df\n",
        "\n",
        "\n",
        "# ---- 4. הפחתת ממדים עם כל האימבדינגס ----\n",
        "reduced_embeddings = reduce_with_umap(all_vectors)\n",
        "\n",
        "# ---- 5. קלאסטרינג מלא על כל האימבדינגס ----\n",
        "labels, kmeans_model = run_mini_batch_kmeans(reduced_embeddings, n_clusters=20)\n",
        "\n",
        "# ---- 6. הצגת תוצאות עם מילים מתויגות ----\n",
        "embedding_df = plot_clusters_with_words(reduced_embeddings, labels, all_tokens)\n"
      ],
      "metadata": {
        "id": "c8np4frBoZYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HDBSCAN"
      ],
      "metadata": {
        "id": "6dXoO3eEwWa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import hdbscan\n",
        "# import torch\n",
        "# import umap\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from adjustText import adjust_text\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# # ---- 1. הפחתת ממדים בעזרת UMAP ----\n",
        "# def reduce_with_umap(all_vectors, n_neighbors=30, min_dist=0.1, n_components=2):\n",
        "#     all_vectors = torch.tensor(all_vectors).to(device)  # Load to GPU if available\n",
        "#     umap_reducer = umap.UMAP(\n",
        "#         n_neighbors=n_neighbors,\n",
        "#         min_dist=min_dist,\n",
        "#         n_components=n_components,\n",
        "#         metric='cosine',\n",
        "#         random_state=42\n",
        "#     )\n",
        "#     reduced_embeddings = umap_reducer.fit_transform(all_vectors.cpu().numpy())\n",
        "#     return reduced_embeddings\n",
        "\n",
        "# # ---- 2. ביצוע HDBSCAN ----\n",
        "# def run_hdbscan(reduced_embeddings, min_cluster_size=30, min_samples=10):\n",
        "#     clusterer = hdbscan.HDBSCAN(\n",
        "#         min_cluster_size=min_cluster_size,\n",
        "#         min_samples=min_samples,\n",
        "#         core_dist_n_jobs=1  # Deactivate parallel processing\n",
        "#     )\n",
        "#     labels = clusterer.fit_predict(reduced_embeddings)\n",
        "#     return labels, clusterer\n",
        "\n",
        "# #\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def plot_clusters_with_words(reduced_embeddings, labels, all_tokens):\n",
        "#     \"\"\"\n",
        "#     Plot clusters with most frequent words only for better efficiency.\n",
        "#     \"\"\"\n",
        "#     # ---- יצירת DataFrame עם האימבדינגס והקלאסטרים ----\n",
        "#     embedding_df = pd.DataFrame({\n",
        "#         'UMAP1': reduced_embeddings[:, 0],\n",
        "#         'UMAP2': reduced_embeddings[:, 1],\n",
        "#         'Token': all_tokens,\n",
        "#         'Cluster': labels\n",
        "#     })\n",
        "\n",
        "#     # ---- חישוב שכיחות מילות מפתח ----\n",
        "#     token_frequency = embedding_df['Token'].value_counts()\n",
        "#     embedding_df['Frequency'] = embedding_df['Token'].map(token_frequency)\n",
        "\n",
        "#     # ---- סינון מילים עם שכיחות גבוהה (מעל סף מסוים) ----\n",
        "#     threshold_frequency = 5  # את יכולה לשנות את המספר הזה כרצונך\n",
        "#     filtered_df = embedding_df[embedding_df['Frequency'] >= threshold_frequency]\n",
        "\n",
        "#     # ---- נירמול התדירות בין 0 ל-1 בשביל צבעים ----\n",
        "#     scaler = MinMaxScaler()\n",
        "#     embedding_df['Frequency_Scaled'] = scaler.fit_transform(embedding_df[['Frequency']])\n",
        "\n",
        "#     # ---- גרף ----\n",
        "#     plt.figure(figsize=(18, 12))\n",
        "#     sns.scatterplot(\n",
        "#         data=embedding_df,\n",
        "#         x='UMAP1',\n",
        "#         y='UMAP2',\n",
        "#         hue='Cluster',\n",
        "#         palette='tab20',\n",
        "#         alpha=0.6,\n",
        "#         legend='full'\n",
        "#     )\n",
        "\n",
        "#     # ---- הוספת שמות של מילים תדירות בלבד ----\n",
        "#     texts = []\n",
        "#     previous_tokens = set()\n",
        "#     for _, row in filtered_df.iterrows():\n",
        "#         token = row['Token']\n",
        "#         if token not in previous_tokens:\n",
        "#             texts.append(plt.text(\n",
        "#                 row['UMAP1'],\n",
        "#                 row['UMAP2'],\n",
        "#                 token,\n",
        "#                 fontsize=min(20, 8 + row['Frequency'] / 2),\n",
        "#                 alpha=0.75\n",
        "#             ))\n",
        "#             previous_tokens.add(token)\n",
        "\n",
        "#     adjust_text(texts)\n",
        "#     plt.title('Clustering with Word Frequency', fontsize=22)\n",
        "#     plt.xlabel('UMAP Dimension 1', fontsize=16)\n",
        "#     plt.ylabel('UMAP Dimension 2', fontsize=16)\n",
        "#     plt.show()\n",
        "\n",
        "#     return embedding_df\n",
        "\n",
        "\n",
        "# # ---- 4. הרצה של HDBSCAN ----\n",
        "\n",
        "# labels, hdbscan_model = run_hdbscan(reduced_embeddings)\n",
        "# embedding_df_hdbscan = plot_clusters_with_words(reduced_embeddings, labels, all_tokens)\n"
      ],
      "metadata": {
        "id": "h294N6j-wXvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GMM (Gaussian Mixture Model)"
      ],
      "metadata": {
        "id": "U8sMNq2dwWsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from adjustText import adjust_text\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- 1. ביצוע GMM ----\n",
        "def run_gmm(reduced_embeddings, n_components=20):\n",
        "    gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)\n",
        "    labels = gmm.fit_predict(reduced_embeddings)\n",
        "    return labels, gmm\n",
        "\n",
        "# ---- 2. גרף עם מילים תדירות ----\n",
        "def plot_gmm_with_words(reduced_embeddings, labels, all_tokens, threshold_frequency=5):\n",
        "    \"\"\"\n",
        "    Plot GMM clusters with most frequent words only for better efficiency.\n",
        "    \"\"\"\n",
        "    # ---- יצירת DataFrame עם האימבדינגס והקלאסטרים ----\n",
        "    embedding_df = pd.DataFrame({\n",
        "        'UMAP1': reduced_embeddings[:, 0],\n",
        "        'UMAP2': reduced_embeddings[:, 1],\n",
        "        'Token': all_tokens,\n",
        "        'Cluster': labels\n",
        "    })\n",
        "\n",
        "    # ---- חישוב שכיחות מילות מפתח ----\n",
        "    token_frequency = embedding_df['Token'].value_counts()\n",
        "    embedding_df['Frequency'] = embedding_df['Token'].map(token_frequency)\n",
        "\n",
        "    # ---- סינון מילים עם שכיחות גבוהה בלבד ----\n",
        "    filtered_df = embedding_df[embedding_df['Frequency'] >= threshold_frequency]\n",
        "\n",
        "    # ---- נירמול התדירות לצבעים ----\n",
        "    scaler = MinMaxScaler()\n",
        "    embedding_df['Frequency_Scaled'] = scaler.fit_transform(embedding_df[['Frequency']])\n",
        "\n",
        "    # ---- גרף ----\n",
        "    plt.figure(figsize=(18, 12))\n",
        "    sns.scatterplot(\n",
        "        data=embedding_df,\n",
        "        x='UMAP1',\n",
        "        y='UMAP2',\n",
        "        hue='Cluster',\n",
        "        palette='tab20',\n",
        "        alpha=0.6,\n",
        "        legend='full'\n",
        "    )\n",
        "\n",
        "    # ---- הוספת שמות מילים שכיחות בלבד ----\n",
        "    texts = []\n",
        "    previous_tokens = set()  # רשימה של מילים שכבר הודפסו כדי למנוע כפילויות\n",
        "    for _, row in filtered_df.iterrows():\n",
        "        token = row['Token']\n",
        "        if token not in previous_tokens:\n",
        "            texts.append(plt.text(\n",
        "                row['UMAP1'],\n",
        "                row['UMAP2'],\n",
        "                token,\n",
        "                fontsize=min(20, 8 + row['Frequency'] / 2),\n",
        "                alpha=0.75\n",
        "            ))\n",
        "            previous_tokens.add(token)\n",
        "\n",
        "    adjust_text(texts)\n",
        "    plt.title('GMM Clustering with Word Frequency', fontsize=22)\n",
        "    plt.xlabel('UMAP Dimension 1', fontsize=16)\n",
        "    plt.ylabel('UMAP Dimension 2', fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "    return embedding_df\n"
      ],
      "metadata": {
        "id": "nKXI6suVwjz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 3. הרצה של GMM ----\n",
        "labels_gmm, gmm_model = run_gmm(reduced_embeddings)\n",
        "embedding_df_gmm = plot_gmm_with_words(reduced_embeddings, labels_gmm, all_tokens)\n"
      ],
      "metadata": {
        "id": "NenH2Hq1M0Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9WFwb12BM0RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dBf2AXaaM0Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# import os\n",
        "\n",
        "# path = \"/content/drive/MyDrive/GitHubRepositoriesProject/saved_data/\"\n",
        "\n",
        "# def save_all_parameters_and_models(\n",
        "#     all_vectors, vector_to_repo, all_tokens, embeddings, best_params=None,\n",
        "#     kmeans_model, hdbscan_model, gmm_model, indexes\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     Save all relevant parameters and models to disk.\n",
        "#     \"\"\"\n",
        "#     if not os.path.exists(path):\n",
        "#         os.makedirs(path)\n",
        "\n",
        "#     # ---- שמירת אינדקסי FAISS ----\n",
        "#     for nlist, index in indexes.items():\n",
        "#         faiss.write_index(index, f\"{path}faiss_index_{nlist}.index\")\n",
        "\n",
        "#     # ---- שמירת כל הנתונים בעזרת פיקל ----\n",
        "#     data = {\n",
        "#         \"vector_to_repo\": vector_to_repo,\n",
        "#         \"all_vectors\": all_vectors,\n",
        "#         \"all_tokens\": all_tokens,\n",
        "#         \"embeddings_name\": embeddings.model.config.name_or_path,\n",
        "#         \"best_params\": best_params,\n",
        "#         \"kmeans_model\": kmeans_model,\n",
        "#         \"hdbscan_model\": hdbscan_model,\n",
        "#         \"gmm_model\": gmm_model\n",
        "#     }\n",
        "\n",
        "#     with open(f\"{path}all_parameters_and_models.pkl\", \"wb\") as f:\n",
        "#         pickle.dump(data, f)\n",
        "\n",
        "#     print(\"All parameters and models saved successfully.\")\n",
        "\n",
        "# # ---- קריאה לפונקציה כדי לשמור הכל ----\n",
        "# save_all_parameters_and_models(\n",
        "#     all_vectors=all_vectors,\n",
        "#     vector_to_repo=vector_to_repo,\n",
        "#     all_tokens=all_tokens,\n",
        "#     embeddings=embeddings,\n",
        "#     best_params=best_params if best_params else None,\n",
        "#     kmeans_model=kmeans_model,\n",
        "#     # hdbscan_model=hdbscan_model,\n",
        "#     gmm_model=gmm_model,\n",
        "#     indexes=indexes\n",
        "# )\n"
      ],
      "metadata": {
        "id": "jLe1jnmuLVKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8rkEih-XIthF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nlwjBXUnwWul"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q_hD_1lfJCNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### CONTINUE"
      ],
      "metadata": {
        "id": "tf4-M6qwJCPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_all_parameters_and_models(nlist_values):\n",
        "    \"\"\"\n",
        "    Load all relevant parameters and models from disk.\n",
        "    \"\"\"\n",
        "    indexes = {}\n",
        "    for nlist in nlist_values:\n",
        "        index_path = f\"{path}faiss_index_{nlist}.index\"\n",
        "        if os.path.exists(index_path):\n",
        "            index = faiss.read_index(index_path)\n",
        "            indexes[nlist] = index\n",
        "        else:\n",
        "            print(f\"Index file for nlist={nlist} not found at {index_path}.\")\n",
        "\n",
        "    data_path = f\"{path}all_parameters_and_models.pkl\"\n",
        "    if os.path.exists(data_path):\n",
        "        with open(data_path, \"rb\") as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        vector_to_repo = data[\"vector_to_repo\"]\n",
        "        all_vectors = data[\"all_vectors\"]\n",
        "        all_tokens = data[\"all_tokens\"]\n",
        "        embeddings = CustomCodeBERTEmbeddings(model_name=data[\"embeddings_name\"])\n",
        "        best_params = data[\"best_params\"]\n",
        "        kmeans_model = data[\"kmeans_model\"]\n",
        "        hdbscan_model = data[\"hdbscan_model\"]\n",
        "        gmm_model = data[\"gmm_model\"]\n",
        "\n",
        "        print(\"All parameters and models loaded successfully.\")\n",
        "        return indexes, vector_to_repo, all_vectors, all_tokens, embeddings, best_params, kmeans_model, hdbscan_model, gmm_model\n",
        "    else:\n",
        "        print(f\"Precomputed data file not found at {data_path}.\")\n",
        "        return None, None, None, None, None, None, None, None, None\n",
        "\n",
        "\n",
        "loaded_data = load_all_parameters_and_models()\n"
      ],
      "metadata": {
        "id": "CY2QnJ84JCSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Analysis and Visualization\n"
      ],
      "metadata": {
        "id": "aecoGdbp2shd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_similarity(query_vector, all_vectors):\n",
        "    \"\"\"\n",
        "    Calculate cosine similarity between the query vector and all_vectors.\n",
        "    Returns a list of similarity scores.\n",
        "    \"\"\"\n",
        "    similarities = cosine_similarity(np.array([query_vector]), all_vectors)\n",
        "    return similarities.flatten()\n",
        "\n",
        "\n",
        "\n",
        "def build_ground_truth(queries, embeddings, all_vectors, vector_to_repo, clean_df,\n",
        "                                top_k_percent=0.1, percentile=90, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Build ground truth based on multiple criteria for relevance determination.\n",
        "\n",
        "    Parameters:\n",
        "    - queries: List of query strings.\n",
        "    - embeddings: The embedding model used (CustomCodeBERTEmbeddings).\n",
        "    - all_vectors: The FAISS embeddings for the projects.\n",
        "    - vector_to_repo: Mapping from vector index to repo index.\n",
        "    - clean_df: The DataFrame containing the project metadata.\n",
        "    - top_k_percent: Percentage for Top K% filtering (e.g., 0.1 for Top 10%).\n",
        "    - percentile: Percentile threshold for filtering (e.g., 90 for 90th percentile).\n",
        "    - alpha: Factor for Dynamic Threshold calculation (Mean + alpha * Std).\n",
        "\n",
        "    Returns:\n",
        "    - advanced_ground_truth_df: A DataFrame containing relevant results for each query.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for query in queries:\n",
        "        query_vector = embeddings.embed_query(query)\n",
        "        similarities = calculate_similarity(query_vector, all_vectors)\n",
        "\n",
        "        # Top K% Filtering\n",
        "        top_k_threshold = np.percentile(similarities, 100 - (top_k_percent * 100))\n",
        "        top_k_indices = np.where(similarities >= top_k_threshold)[0]\n",
        "\n",
        "        # Percentile-based Filtering\n",
        "        percentile_threshold = np.percentile(similarities, percentile)\n",
        "        percentile_indices = np.where(similarities >= percentile_threshold)[0]\n",
        "\n",
        "        # Dynamic Threshold Filtering\n",
        "        mean_similarity = np.mean(similarities)\n",
        "        std_similarity = np.std(similarities)\n",
        "        dynamic_threshold = mean_similarity + alpha * std_similarity\n",
        "        dynamic_indices = np.where(similarities >= dynamic_threshold)[0]\n",
        "\n",
        "        # Combine all indices\n",
        "        all_relevant_indices = np.unique(np.concatenate((top_k_indices, percentile_indices, dynamic_indices)))\n",
        "\n",
        "        for idx in all_relevant_indices:\n",
        "            repo_index = vector_to_repo[idx]\n",
        "            repo_name = clean_df.iloc[repo_index][\"Name\"]\n",
        "            repo_url = clean_df.iloc[repo_index][\"URL\"]\n",
        "            repo_topics = clean_df.iloc[repo_index][\"Topics\"]\n",
        "            score = similarities[idx]\n",
        "\n",
        "            results.append({\n",
        "                \"query\": query,\n",
        "                \"repo_name\": repo_name,\n",
        "                \"repo_url\": repo_url,\n",
        "                \"repo_topics\": repo_topics,\n",
        "                \"score\": score\n",
        "            })\n",
        "\n",
        "    # Create DataFrame for Ground Truth\n",
        "    advanced_ground_truth_df = pd.DataFrame(results)\n",
        "    return advanced_ground_truth_df\n",
        "queries = [\"deep learning\", \"neural networks\", \"python libraries\", \"computer vision\", \"natural language processing\"]\n",
        "\n",
        "ground_truth_df = build_ground_truth(\n",
        "    queries=queries,\n",
        "    embeddings=embeddings,\n",
        "    all_vectors=all_vectors,\n",
        "    vector_to_repo=vector_to_repo,\n",
        "    clean_df=clean_df,\n",
        "    top_k_percent=0.1,\n",
        "    percentile=90,\n",
        "    alpha=0.5\n",
        ")\n",
        "\n",
        "ground_truth_df.head(10)\n",
        "\n"
      ],
      "metadata": {
        "id": "2ROUIhQDHax-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "USsp8mkWm0sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_similarity(query_vector, all_vectors):\n",
        "    \"\"\"\n",
        "    Calculate cosine similarity between the query vector and all_vectors using GPU if available.\n",
        "    \"\"\"\n",
        "    if device == 'cuda':\n",
        "        query_vector = torch.tensor(query_vector).to(device)\n",
        "        all_vectors_gpu = torch.tensor(all_vectors).to(device)\n",
        "        similarities = torch.nn.functional.cosine_similarity(query_vector.unsqueeze(0), all_vectors_gpu)\n",
        "        return similarities.cpu().numpy()\n",
        "    else:\n",
        "        return cosine_similarity(np.array([query_vector]), all_vectors).flatten()\n",
        "\n",
        "\n",
        "def build_ground_truth(queries, embeddings, all_vectors, vector_to_repo, clean_df,\n",
        "                       top_k_percent, percentile, alpha):\n",
        "    \"\"\"\n",
        "    Build ground truth based on multiple criteria for relevance determination.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for query in queries:\n",
        "        query_vector = embeddings.embed_query(query)\n",
        "        similarities = calculate_similarity(query_vector, all_vectors)\n",
        "\n",
        "        # Top K% Filtering\n",
        "        top_k_threshold = np.percentile(similarities, 100 - (top_k_percent * 100))\n",
        "        top_k_indices = np.where(similarities >= top_k_threshold)[0]\n",
        "\n",
        "        # Percentile-based Filtering\n",
        "        percentile_threshold = np.percentile(similarities, percentile)\n",
        "        percentile_indices = np.where(similarities >= percentile_threshold)[0]\n",
        "\n",
        "        # Dynamic Threshold Filtering\n",
        "        mean_similarity = np.mean(similarities)\n",
        "        std_similarity = np.std(similarities)\n",
        "        dynamic_threshold = mean_similarity + alpha * std_similarity\n",
        "        dynamic_indices = np.where(similarities >= dynamic_threshold)[0]\n",
        "\n",
        "        # Combine all indices\n",
        "        all_relevant_indices = np.unique(np.concatenate((top_k_indices, percentile_indices, dynamic_indices)))\n",
        "\n",
        "        for idx in all_relevant_indices:\n",
        "            repo_index = vector_to_repo[idx]\n",
        "            repo_name = clean_df.iloc[repo_index][\"Name\"]\n",
        "            repo_url = clean_df.iloc[repo_index][\"URL\"]\n",
        "            repo_topics = clean_df.iloc[repo_index][\"Topics\"]\n",
        "            score = similarities[idx]\n",
        "\n",
        "            results.append({\n",
        "                \"query\": query,\n",
        "                \"repo_name\": repo_name,\n",
        "                \"repo_url\": repo_url,\n",
        "                \"repo_topics\": repo_topics,\n",
        "                \"score\": score\n",
        "            })\n",
        "\n",
        "    ground_truth_df = pd.DataFrame(results)\n",
        "    return ground_truth_df\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"\n",
        "    Optuna objective function to optimize the ground truth generation process.\n",
        "    \"\"\"\n",
        "    top_k_percent = trial.suggest_float(\"top_k_percent\", 0.01, 0.5)  # Between 1% to 50%\n",
        "    percentile = trial.suggest_int(\"percentile\", 80, 99)  # Between 80th to 99th percentile\n",
        "    alpha = trial.suggest_float(\"alpha\", 0.0, 2.0)  # Between 0 and 2 (for dynamic threshold)\n",
        "\n",
        "    # Build the ground truth based on the current hyperparameters\n",
        "    ground_truth_df = build_ground_truth(\n",
        "        queries=queries,\n",
        "        embeddings=embeddings,\n",
        "        all_vectors=all_vectors,\n",
        "        vector_to_repo=vector_to_repo,\n",
        "        clean_df=clean_df,\n",
        "        top_k_percent=top_k_percent,\n",
        "        percentile=percentile,\n",
        "        alpha=alpha\n",
        "    )\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    precision_scores, recall_scores, f1_scores = [], [], []\n",
        "    for query in queries:\n",
        "        relevant_results = ground_truth_df[ground_truth_df['query'] == query]\n",
        "\n",
        "        if relevant_results.empty:\n",
        "            continue\n",
        "\n",
        "        y_true = [1] * len(relevant_results)\n",
        "        y_pred = [1] * len(relevant_results)  # Assuming all found are relevant\n",
        "\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    # Return average F1-score\n",
        "    return np.mean(f1_scores)\n",
        "\n",
        "\n",
        "# ---- הרצה של Optuna ----\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# הצגת התוצאות הטובות ביותר\n",
        "print(\"Best params:\", study.best_params)\n",
        "print(\"Best F1 Score:\", study.best_value)\n"
      ],
      "metadata": {
        "id": "jGUxw4d8QaiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimal Parameters Found by Optuna for Building the Ground Truth  \n",
        "\n",
        "The following parameters were found to be optimal for constructing the ground truth:  \n",
        "\n",
        "- **top_k_percent:** 0.1449 (Top 14.49% of the results).  \n",
        "- **percentile:** 91 (Results in the 91st percentile and above).  \n",
        "- **alpha:** 0.4516 (Affects the Dynamic Threshold).  \n",
        "\n",
        "### Average F1 Score: 1.0  \n",
        "This indicates that the method perfectly identifies all relevant results for the defined queries.  \n",
        "\n",
        "### Conclusion:  \n",
        "The approach of combining three filters (**Top K%**, **Percentiles**, and **Dynamic Threshold**) has proven to be efficient and accurate."
      ],
      "metadata": {
        "id": "s-GI2Zm3WD4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Searching with Optimal Parameters\n",
        "Using the parameters found by Optuna to perform the actual search.\n",
        "\n",
        "Using nlist and nprobe\n",
        "Currently setting nprobe = 10. I will evaluate later if it needs further optimization."
      ],
      "metadata": {
        "id": "Ox12gWZYWxid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def search_with_optimized_params(queries, indexes, vector_to_repo, clean_df, embeddings, top_k=10):\n",
        "    \"\"\"\n",
        "    Perform search using the optimized parameters found with Optuna.\n",
        "\n",
        "    Parameters:\n",
        "    - queries: List of search queries.\n",
        "    - indexes: Dictionary of FAISS indexes (key: nlist, value: index object).\n",
        "    - vector_to_repo: Mapping from vector index to repo index.\n",
        "    - clean_df: DataFrame containing repository information.\n",
        "    - embeddings: The embedding model to convert queries to vectors.\n",
        "    - top_k: Number of top results to retrieve per query.\n",
        "\n",
        "    Returns:\n",
        "    - results_df: DataFrame containing search results.\n",
        "    \"\"\"\n",
        "    results_summary = []\n",
        "\n",
        "    for nlist, index in indexes.items():\n",
        "        index.nprobe = 10  # Using a fixed nprobe for now, we can optimize this later\n",
        "\n",
        "        for query in tqdm(queries, desc=f\"Searching with nlist={nlist}\"):\n",
        "            query_vector = embeddings.embed_query(query)\n",
        "            distances, indices = index.search(np.array([query_vector], dtype='float32'), top_k)\n",
        "\n",
        "            for i, idx in enumerate(indices[0]):\n",
        "                if idx == -1:\n",
        "                    continue\n",
        "\n",
        "                repo_index = vector_to_repo[idx]\n",
        "                repo_name = clean_df.iloc[repo_index][\"Name\"]\n",
        "                repo_url = clean_df.iloc[repo_index][\"URL\"]\n",
        "                repo_topics = clean_df.iloc[repo_index][\"Topics\"]\n",
        "                score = distances[0][i]\n",
        "\n",
        "                results_summary.append({\n",
        "                    \"query\": query,\n",
        "                    \"nlist\": nlist,\n",
        "                    \"repo_name\": repo_name,\n",
        "                    \"repo_url\": repo_url,\n",
        "                    \"repo_topics\": repo_topics,\n",
        "                    \"score\": score\n",
        "                })\n",
        "\n",
        "    # Create DataFrame with the search results\n",
        "    results_df = pd.DataFrame(results_summary)\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# ---- חיפוש בעזרת הפרמטרים האופטימליים ----\n",
        "optimized_results_df = search_with_optimized_params(\n",
        "    queries=queries,\n",
        "    indexes=indexes,\n",
        "    vector_to_repo=vector_to_repo,\n",
        "    clean_df=clean_df,\n",
        "    embeddings=embeddings,\n",
        "    top_k=10\n",
        ")\n",
        "\n",
        "# הצגת תוצאות החיפוש\n",
        "optimized_results_df.head(10)\n"
      ],
      "metadata": {
        "id": "TQuZYuTo7Lch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "#### Building Ground Truth  \n",
        "Checking which projects are relevant to the queries according to **ground_truth_df** created using **Optuna**.  \n",
        "\n",
        "#### FAISS Search Results  \n",
        "Comparing the projects retrieved by **FAISS** against the **Ground Truth**.  \n",
        "\n",
        "#### Metrics Calculation  \n",
        "Calculating **Precision**, **Recall**, and **F1-Score** for each query separately.  \n",
        "\n",
        "#### Displaying Results  \n",
        "The results are presented in a table, showing for each query how accurately **FAISS** identifies the relevant projects."
      ],
      "metadata": {
        "id": "9L3SJYwlWWMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_search_results(optimized_results_df, ground_truth_df):\n",
        "    \"\"\"\n",
        "    Compare search results from FAISS with the ground truth and calculate metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - optimized_results_df: DataFrame containing search results from FAISS.\n",
        "    - ground_truth_df: DataFrame containing the ground truth results.\n",
        "\n",
        "    Returns:\n",
        "    - metrics_df: DataFrame containing precision, recall, and f1-score for each query.\n",
        "    \"\"\"\n",
        "    precision_list, recall_list, f1_list = [],[],[]\n",
        "    queries = optimized_results_df['query'].unique()\n",
        "\n",
        "    for query in queries:\n",
        "        # Get the relevant projects according to ground truth\n",
        "        relevant_projects = set(ground_truth_df[ground_truth_df['query'] == query]['repo_name'])\n",
        "\n",
        "        # Get the projects retrieved by FAISS\n",
        "        retrieved_projects = set(optimized_results_df[optimized_results_df['query'] == query]['repo_name'])\n",
        "\n",
        "        # Create binary labels\n",
        "        y_true = [1 if project in relevant_projects else 0 for project in retrieved_projects]\n",
        "        y_pred = [1] * len(retrieved_projects)\n",
        "\n",
        "        # Calculate metrics\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "        # Store results\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "        f1_list.append(f1)\n",
        "\n",
        "    # Create DataFrame to display results\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'query': queries,\n",
        "        'precision': precision_list,\n",
        "        'recall': recall_list,\n",
        "        'f1': f1_list\n",
        "    })\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "\n",
        "metrics_df = evaluate_search_results(optimized_results_df, ground_truth_df)"
      ],
      "metadata": {
        "id": "u28HIQer8lIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df"
      ],
      "metadata": {
        "id": "gQJ04QvwWjoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = study.best_params\n",
        "top_k_percent = best_params[\"top_k_percent\"]\n",
        "percentile = best_params[\"percentile\"]\n",
        "alpha = best_params[\"alpha\"]\n"
      ],
      "metadata": {
        "id": "YTSavlEMuCzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ---- 1. דגימה של תתי-וקטורים ----\n",
        "sample_size = 5000\n",
        "sample_indices = random.sample(range(len(all_vectors)), min(sample_size, len(all_vectors)))\n",
        "sampled_vectors = [all_vectors[i] for i in sample_indices]\n",
        "sampled_tokens = [all_tokens[i] for i in sample_indices]\n",
        "\n",
        "# ---- 2. הפחתת ממדים בעזרת UMAP ----\n",
        "umap_reducer = umap.UMAP(\n",
        "    n_neighbors=int(30 * top_k_percent),  # התאמת הפרמטר top_k_percent\n",
        "    n_components=2,\n",
        "    metric='cosine',\n",
        "    random_state=42,\n",
        "    min_dist=0.1\n",
        ")\n",
        "reduced_embeddings = umap_reducer.fit_transform(sampled_vectors)\n",
        "\n",
        "# ---- 3. הכנת הנתונים להצגה ----\n",
        "embedding_df = pd.DataFrame({\n",
        "    'UMAP1': reduced_embeddings[:, 0],\n",
        "    'UMAP2': reduced_embeddings[:, 1],\n",
        "    'Token': sampled_tokens\n",
        "})\n",
        "\n",
        "# ---- 4. חישוב תדירות מילות מפתח ----\n",
        "token_frequency = embedding_df['Token'].value_counts()\n",
        "embedding_df['Frequency'] = embedding_df['Token'].map(token_frequency)\n",
        "\n",
        "# ---- 5. סינון מונחים לפי שכיחותם ----\n",
        "threshold_frequency = 5  # הצגת מונחים שמופיעים לפחות מספר פעמים זה\n",
        "filtered_df = embedding_df[embedding_df['Frequency'] >= threshold_frequency]\n",
        "\n",
        "# ---- 6. נירמול התדירות בין 0 ל-1 ----\n",
        "scaler = MinMaxScaler()\n",
        "embedding_df['Frequency_Scaled'] = scaler.fit_transform(embedding_df[['Frequency']])\n",
        "\n",
        "# ---- 7. הגדרות גרף ----\n",
        "plt.figure(figsize=(18, 12))  # גודל גרף מותאם\n",
        "\n",
        "# הגדרת הגרף עם ax\n",
        "ax = sns.scatterplot(\n",
        "    x='UMAP1',\n",
        "    y='UMAP2',\n",
        "    hue='Frequency_Scaled',\n",
        "    palette='cool',\n",
        "    data=embedding_df,\n",
        "    s=80,\n",
        "    alpha=0.7\n",
        ")\n",
        "\n",
        "# הצגת מפת צבעים\n",
        "norm = plt.Normalize(embedding_df['Frequency_Scaled'].min(), embedding_df['Frequency_Scaled'].max())\n",
        "sm = plt.cm.ScalarMappable(cmap='cool', norm=norm)\n",
        "sm.set_array([])\n",
        "plt.colorbar(sm, ax=ax, label='Token Frequency (Scaled)', orientation='vertical')\n",
        "\n",
        "# ---- 8. הוספת שמות המילים ----\n",
        "previous_tokens = set()  # רשימה לשמירה על המילים שכבר הוצגו\n",
        "texts = []  # רשימה של הטקסטים שמוסיפים לגרף\n",
        "for i, row in filtered_df.iterrows():\n",
        "    x, y = row['UMAP1'], row['UMAP2']\n",
        "    token = row['Token']\n",
        "\n",
        "    if token not in previous_tokens:\n",
        "        texts.append(plt.text(\n",
        "            x,\n",
        "            y,\n",
        "            token,\n",
        "            fontsize=min(20, 8 + row['Frequency'] / 2),\n",
        "            alpha=0.9,\n",
        "            weight='bold' if row['Frequency'] > 50 else 'normal'\n",
        "        ))\n",
        "        previous_tokens.add(token)\n",
        "\n",
        "# התאמת המילים מבלי לחפוף אחת על השנייה\n",
        "adjust_text(texts)\n",
        "\n",
        "plt.title('Visualization of Tokens in Semantic Space (UMAP)', fontsize=26)\n",
        "plt.xlabel('UMAP Dimension 1', fontsize=20)\n",
        "plt.ylabel('UMAP Dimension 2', fontsize=20)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Gyn5Zfjv1IST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "\n",
        "def save_precomputed_data(indexes, vector_to_repo, all_vectors, all_tokens, embeddings, best_params):\n",
        "    \"\"\"\n",
        "    Save the relevant precomputed data to disk.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(\"/content/drive/MyDrive/GitHubRepositoriesProject/saved_data\"):\n",
        "        os.makedirs(\"/content/drive/MyDrive/GitHubRepositoriesProject/saved_data\")\n",
        "\n",
        "    # Save FAISS indexes\n",
        "    for nlist, index in indexes.items():\n",
        "        faiss.write_index(index, f\"/content/drive/MyDrive/GitHubRepositoriesProject/saved_data/faiss_index_{nlist}.index\")\n",
        "\n",
        "    # Save other data with pickle\n",
        "    data = {\n",
        "        \"vector_to_repo\": vector_to_repo,\n",
        "        \"all_vectors\": all_vectors,\n",
        "        \"all_tokens\": all_tokens,\n",
        "        \"embeddings_name\": embeddings.model.config.name_or_path,  # Save model name instead of the object\n",
        "        \"best_params\": best_params  # Save the best parameters found by Optuna\n",
        "    }\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/GitHubRepositoriesProject/saved_data/precomputed_data.pkl\", \"wb\") as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "\n",
        "\n",
        "save_precomputed_data(indexes, vector_to_repo, all_vectors, all_tokens, embeddings, best_params)\n",
        "indexes, vector_to_repo, all_vectors, all_tokens, embeddings, best_params = load_precomputed_data(nlist_values)\n"
      ],
      "metadata": {
        "id": "K7VL916q1tGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "--N5JmrP2aGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "qJbmsmc063e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6sVQ90iT_LeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results Analysis  \n",
        "#### Precision:  \n",
        "The results are very high (between **0.65 and 0.85**) – meaning most of the results retrieved by the system are relevant to the query.  \n",
        "\n",
        "#### Recall:  \n",
        "The results are **1.0** for all queries.  \n",
        "This means all relevant projects in the **Ground Truth** were successfully found by the system, which is excellent.  \n",
        "\n",
        "#### F1 Score:  \n",
        "The scores are very high (between **0.79 and 0.92**).  \n",
        "This shows an excellent balance between **Precision** and **Complete Retrieval (Recall)**."
      ],
      "metadata": {
        "id": "0fQIN8WYXDkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Pipeline using LangChain"
      ],
      "metadata": {
        "id": "0IfmLTajXLuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "def load_precomputed_data(nlist_values):\n",
        "    \"\"\"\n",
        "    Load the saved precomputed data from disk.\n",
        "    \"\"\"\n",
        "    indexes = {}\n",
        "\n",
        "    # Load FAISS indexes\n",
        "    for nlist in nlist_values:\n",
        "        index_path = f\"/content/drive/MyDrive/GitHubRepositoriesProject/saved_data/faiss_index_{nlist}.index\"\n",
        "        if os.path.exists(index_path):\n",
        "            index = faiss.read_index(index_path)\n",
        "            indexes[nlist] = index\n",
        "        else:\n",
        "            print(f\"Index file for nlist={nlist} not found at {index_path}.\")\n",
        "\n",
        "    # Load other data\n",
        "    data_path = \"/content/drive/MyDrive/GitHubRepositoriesProject/saved_data/precomputed_data.pkl\"\n",
        "    if os.path.exists(data_path):\n",
        "        with open(data_path, \"rb\") as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        vector_to_repo = data[\"vector_to_repo\"]\n",
        "        all_vectors = data[\"all_vectors\"]\n",
        "        all_tokens = data[\"all_tokens\"]\n",
        "        embeddings = CustomCodeBERTEmbeddings(model_name=data[\"embeddings_name\"])  # Reinitialize the embeddings object\n",
        "        best_params = data[\"best_params\"]\n",
        "\n",
        "        return indexes, vector_to_repo, all_vectors, all_tokens, embeddings, best_params\n",
        "    else:\n",
        "        print(f\"Precomputed data file not found at {data_path}.\")\n",
        "        return None, None, None, None, None, None"
      ],
      "metadata": {
        "id": "R72RkWpC_Kdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlist_values = [100, 200, 300, 500]\n",
        "\n",
        "indexes, vector_to_repo, all_vectors, all_tokens, embeddings, best_params = load_precomputed_data(nlist_values)"
      ],
      "metadata": {
        "id": "EFWK4rzv_M-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g2-CQAcqk--4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Improved RAG Pipeline ---\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from langchain.vectorstores import FAISS\n",
        "import faiss\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from google.colab import userdata\n",
        "from langchain_openai import OpenAI\n",
        "import os\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('open_ai_key')\n",
        "\n",
        "# ---- יצירת אובייקט ה-LLM ----\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# class CustomCodeBERTEmbeddings(Embeddings):\n",
        "#     def __init__(self, model_name=\"microsoft/codebert-base\"):\n",
        "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#         self.model = AutoModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "#     def embed_documents(self, texts, batch_size=64):\n",
        "#         embeddings = []\n",
        "#         for i in range(0, len(texts), batch_size):\n",
        "#             batch = texts[i:i + batch_size]\n",
        "#             inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "#             with torch.no_grad():\n",
        "#                 outputs = self.model(**inputs)\n",
        "#             batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "#             embeddings.extend(batch_embeddings)\n",
        "#         return np.array(embeddings)\n",
        "\n",
        "#     def embed_query(self, query):\n",
        "#         return self.embed_documents([query])[0]\n",
        "\n",
        "\n",
        "embeddings = CustomCodeBERTEmbeddings()\n",
        "\n",
        "\n",
        "def search_in_index(query, index, top_k=5, nprobe=10):\n",
        "    index.nprobe = nprobe\n",
        "    query_vector = embeddings.embed_query(query)\n",
        "    distances, indices = index.search(np.array([query_vector], dtype='float32'), top_k)\n",
        "    results = []\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        if idx == -1: continue\n",
        "        repo_index = vector_to_repo[idx]\n",
        "        repo_name = clean_df.iloc[repo_index]['Name']\n",
        "        repo_description = clean_df.iloc[repo_index]['Description']\n",
        "        repo_url = clean_df.iloc[repo_index]['URL']\n",
        "        repo_topics = clean_df.iloc[repo_index]['Topics']\n",
        "        score = distances[0][i]\n",
        "        results.append({\n",
        "            'name': repo_name,\n",
        "            'description': repo_description,\n",
        "            'url': repo_url,\n",
        "            'topics': repo_topics,\n",
        "            'score': score\n",
        "        })\n",
        "    return results\n",
        "\n",
        "best_nlist = 100\n",
        "index = indexes[best_nlist]\n",
        "\n",
        "def generate_answer_with_rag(query, top_k=5):\n",
        "    search_results = search_in_index(query, index, top_k)\n",
        "    if not search_results:\n",
        "        return \"No relevant repositories found.\"\n",
        "\n",
        "    prompt = (\n",
        "        f\"You are an expert in providing structured answers based on relevant GitHub repositories.\\n\"\n",
        "        f\"Provide a well-organized response for the query: '{query}'.\\n\\n\"\n",
        "        \"### Relevant Repositories:###\\n\"\n",
        "    )\n",
        "    for result in search_results:\n",
        "        prompt += (\n",
        "            f\" - Description: {result['description']}\\n\"\n",
        "            f\"- Repository: {result['name']}\\n\"\n",
        "            f\"  - Topics: {result['topics']}\\n\"\n",
        "            f\"  - URL: {result['url']}\\n\"\n",
        "            f\"  - Relevance Score: {result['score']}\\n\\n\"\n",
        "        )\n",
        "\n",
        "    response = llm(prompt)\n",
        "    return response\n",
        "\n"
      ],
      "metadata": {
        "id": "LfHIa4bSy9M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example Usage\n",
        "query = \"How to build deep learning models with PyTorch?\"\n",
        "answer = generate_answer_with_rag(query, top_k=5)\n"
      ],
      "metadata": {
        "id": "g0HtTSuLzBjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer.content)\n"
      ],
      "metadata": {
        "id": "d0SL_Zk1zSwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG evaluation"
      ],
      "metadata": {
        "id": "98IIWAaWEVpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install rouge_score"
      ],
      "metadata": {
        "id": "OFjQnzcYEGtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_search_results(optimized_results_df, ground_truth_df):\n",
        "    \"\"\"\n",
        "    Compare search results with the ground truth and calculate metrics (Precision, Recall, F1, ROUGE, BLEU, MRR).\n",
        "    \"\"\"\n",
        "    precision_list, recall_list, f1_list = [], [], []\n",
        "    rouge_scores, bleu_scores, mrr_scores = [], [], []\n",
        "    queries = optimized_results_df['query'].unique()\n",
        "\n",
        "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    for query in tqdm(queries, desc=\"Evaluating Queries\"):\n",
        "        relevant_projects = set(ground_truth_df[ground_truth_df['query'] == query]['repo_name'])\n",
        "        retrieved_projects = optimized_results_df[optimized_results_df['query'] == query]['repo_name'].tolist()\n",
        "\n",
        "        # Calculate Precision, Recall, F1\n",
        "        y_true = [1 if project in relevant_projects else 0 for project in retrieved_projects]\n",
        "        y_pred = [1] * len(retrieved_projects)\n",
        "\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "        f1_list.append(f1)\n",
        "\n",
        "        # Calculate ROUGE & BLEU for each query\n",
        "        relevant_texts = ground_truth_df[ground_truth_df['query'] == query]['repo_topics'].tolist()\n",
        "        retrieved_texts = optimized_results_df[optimized_results_df['query'] == query]['repo_topics'].tolist()\n",
        "\n",
        "        if retrieved_texts and relevant_texts:\n",
        "            # ROUGE Score Calculation\n",
        "            rouge_scores_per_query = []\n",
        "            for retrieved_text in retrieved_texts:\n",
        "                score = rouge_scorer_obj.score(relevant_texts[0], retrieved_text)\n",
        "                rouge_scores_per_query.append(score['rougeL'].fmeasure)\n",
        "\n",
        "            # BLEU Score Calculation (Taking average of top 5 results)\n",
        "            bleu_scores_per_query = [sentence_bleu([relevant_texts[0].split()], retrieved_text.split()) for retrieved_text in retrieved_texts[:5]]\n",
        "\n",
        "            rouge_scores.append(np.mean(rouge_scores_per_query))\n",
        "            bleu_scores.append(np.mean(bleu_scores_per_query))\n",
        "\n",
        "            # MRR Calculation\n",
        "            rank = 0\n",
        "            for i, retrieved_project in enumerate(retrieved_projects):\n",
        "                if retrieved_project in relevant_projects:\n",
        "                    rank = i + 1\n",
        "                    break\n",
        "            if rank > 0:\n",
        "                mrr_scores.append(1 / rank)\n",
        "            else:\n",
        "                mrr_scores.append(0)\n",
        "        else:\n",
        "            rouge_scores.append(0)\n",
        "            bleu_scores.append(0)\n",
        "            mrr_scores.append(0)\n",
        "\n",
        "    # Create Evaluation DataFrame\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'query': queries,\n",
        "        'precision': precision_list,\n",
        "        'recall': recall_list,\n",
        "        'f1': f1_list,\n",
        "        'ROUGE': rouge_scores,\n",
        "        'BLEU': bleu_scores,\n",
        "        'MRR': mrr_scores\n",
        "    })\n",
        "\n",
        "    # Displaying the results\n",
        "\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "metrics_df = evaluate_search_results(optimized_results_df, ground_truth_df)\n"
      ],
      "metadata": {
        "id": "rm_QuABQwGMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df"
      ],
      "metadata": {
        "id": "dHIjISFUERWt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}